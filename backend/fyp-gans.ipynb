{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14774,"databundleVersionId":875431,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc \nimport cv2\nimport torch\nimport shutil\nimport random\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import TensorDataset\nfrom torchvision import transforms, datasets, models\nfrom torch.utils.data import DataLoader, Dataset\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nfrom torch import nn \nimport torch.optim as optim\nfrom  collections import OrderedDict\nimport seaborn as sns\nfrom collections import Counter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T11:53:25.850852Z","iopub.execute_input":"2025-01-04T11:53:25.851101Z","iopub.status.idle":"2025-01-04T11:53:31.421369Z","shell.execute_reply.started":"2025-01-04T11:53:25.851074Z","shell.execute_reply":"2025-01-04T11:53:31.420705Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Function to check if an image file is valid\ndef is_image_valid(image_path):\n    try:\n        Image.open(image_path).load()\n        return True\n    except Exception:\n        return False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T11:53:39.350879Z","iopub.execute_input":"2025-01-04T11:53:39.351415Z","iopub.status.idle":"2025-01-04T11:53:39.356514Z","shell.execute_reply.started":"2025-01-04T11:53:39.351364Z","shell.execute_reply":"2025-01-04T11:53:39.355540Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Function to copy images to respective folders\ndef copy_images(df , data, destination_folder,main_folder):\n    data_columns = []\n    for row in df:\n        # adding the first row\n        data_columns.append(row)\n    \n    for index, row in data.iterrows():\n        image_name = row[data_columns[0]]\n        image_name_with_extension = image_name + \".png\"\n        level = str(row[data_columns[1]])\n\n        source_path = os.path.join(main_folder, image_name_with_extension)\n        destination_path = os.path.join(destination_folder, level, image_name_with_extension)\n\n        os.makedirs(os.path.join(destination_folder, level), exist_ok=True)\n\n        # Check if the source file exists and is valid\n        if os.path.isfile(source_path) and is_image_valid(source_path) :\n            try:\n                shutil.copyfile(source_path, destination_path)\n                #print(f\"Copied: {image_name_with_extension}\")\n            except Exception as e:\n                print(f\"Error copying {image_name_with_extension}: {e}\")\n        else:\n            print(f\"Invalid or missing file: {image_name_with_extension}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T11:53:44.367782Z","iopub.execute_input":"2025-01-04T11:53:44.368183Z","iopub.status.idle":"2025-01-04T11:53:44.375945Z","shell.execute_reply.started":"2025-01-04T11:53:44.368151Z","shell.execute_reply":"2025-01-04T11:53:44.375142Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Path to the main folder containing images and the CSV file\nmain_folder ='/kaggle/input/aptos2019-blindness-detection/train_images'\ncsv_file ='/kaggle/input/aptos2019-blindness-detection/train.csv'\n    \n\n# Load CSV file\ndf = pd.read_csv(csv_file)\n\n# Exclude the first class (class 0)\n# df = df[df['diagnosis'] != 0]\n\n# can reduce the dataset to a certain percentage.\ndf = df.sample(frac=1.0, random_state=42)\n\nprint(df.shape)\n\n# Path to the data folder\ndata_folder = \"/kaggle/working/\"\n\n# Create Train and Test folders if not exist\ntrain_folder = os.path.join(data_folder, \"Train\")\ntest_folder = os.path.join(data_folder, \"Test\")\n\nos.makedirs(train_folder, exist_ok=True)\nos.makedirs(test_folder, exist_ok=True)\n\n# Split data into train and test with 80:20 ratio\ntrain_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n\n# Copy images to Train folder\ncopy_images(df, train_data, train_folder, main_folder)\n\n# Copy images to Test folder\ncopy_images(df , test_data, test_folder,main_folder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T11:53:48.819986Z","iopub.execute_input":"2025-01-04T11:53:48.820350Z","iopub.status.idle":"2025-01-04T12:01:08.772505Z","shell.execute_reply.started":"2025-01-04T11:53:48.820318Z","shell.execute_reply":"2025-01-04T12:01:08.771480Z"}},"outputs":[{"name":"stdout","text":"(3662, 2)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"def create_validate_folders(train_path, validate_path):\n    if not os.path.exists(validate_path):\n        os.makedirs(validate_path)\n    for class_folder in os.listdir(train_path):\n        class_folder_path = os.path.join(validate_path, class_folder)\n        if not os.path.exists(class_folder_path):\n            os.makedirs(class_folder_path)\n\ndef move_data(train_path, validate_path, split_ratio=0.1):\n    for class_folder in os.listdir(train_path):\n        class_folder_path = os.path.join(train_path, class_folder)\n        validate_class_folder_path = os.path.join(validate_path, class_folder)\n        \n        files = os.listdir(class_folder_path)\n        num_files_to_move = int(len(files) * split_ratio)\n        files_to_move = random.sample(files, num_files_to_move)\n        \n        for file in files_to_move:\n            src_file_path = os.path.join(class_folder_path, file)\n            dst_file_path = os.path.join(validate_class_folder_path, file)\n            shutil.move(src_file_path, dst_file_path)  # Copy File from Train to Validate\n            #os.remove(src_file_path)  # Delete FIle from train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T12:01:29.802414Z","iopub.execute_input":"2025-01-04T12:01:29.802758Z","iopub.status.idle":"2025-01-04T12:01:29.809481Z","shell.execute_reply.started":"2025-01-04T12:01:29.802724Z","shell.execute_reply":"2025-01-04T12:01:29.808410Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"test_path     = '/kaggle/working/Test'\ntrain_path    = '/kaggle/working/Train'\nvalidate_path = '/kaggle/working/Validate'\n    \ncreate_validate_folders(train_path, validate_path)\nmove_data(train_path, validate_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T12:01:43.252914Z","iopub.execute_input":"2025-01-04T12:01:43.253286Z","iopub.status.idle":"2025-01-04T12:01:43.268300Z","shell.execute_reply.started":"2025-01-04T12:01:43.253255Z","shell.execute_reply":"2025-01-04T12:01:43.267423Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomRotation(degrees=45),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomVerticalFlip(p=0.3),\n    transforms.ColorJitter(brightness=0.01, contrast=0.1, saturation=0.1),\n    transforms.ToTensor(),  # Convert images to PyTorch tensors\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n\n# Transformation for test dataset\nTransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Convert images to PyTorch tensors\n])\n\ntrain_dataset = datasets.ImageFolder(root=train_path, transform=transform)\nval_dataset = datasets.ImageFolder(root=validate_path, transform=Transform)\ntest_dataset = datasets.ImageFolder(root=test_path, transform=Transform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T12:01:46.322087Z","iopub.execute_input":"2025-01-04T12:01:46.322894Z","iopub.status.idle":"2025-01-04T12:01:46.342925Z","shell.execute_reply.started":"2025-01-04T12:01:46.322853Z","shell.execute_reply":"2025-01-04T12:01:46.342281Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"validation_loader1 = DataLoader(val_dataset, batch_size=8, shuffle=True)\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\ntest_loader =  DataLoader(test_dataset, batch_size=8, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T12:01:50.434844Z","iopub.execute_input":"2025-01-04T12:01:50.435558Z","iopub.status.idle":"2025-01-04T12:01:50.440128Z","shell.execute_reply.started":"2025-01-04T12:01:50.435519Z","shell.execute_reply":"2025-01-04T12:01:50.439207Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport csv\nimport torch.nn.functional as F\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# DCGAN Architecture\n\n# Generator model with BatchNorm\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.fc1 = nn.Linear(100, 256)\n        self.bn1 = nn.BatchNorm1d(256)\n        self.fc2 = nn.Linear(256, 512)\n        self.bn2 = nn.BatchNorm1d(512)\n        self.fc3 = nn.Linear(512, 1024)\n        self.bn3 = nn.BatchNorm1d(1024)\n        self.fc4 = nn.Linear(1024, 3*224*224)  # Output image size (3 channels, 224x224)\n\n    def forward(self, z):\n        x = torch.relu(self.bn1(self.fc1(z)))\n        x = torch.relu(self.bn2(self.fc2(x)))\n        x = torch.relu(self.bn3(self.fc3(x)))\n        x = torch.tanh(self.fc4(x))  # Using tanh to normalize between -1 and 1\n        return x.view(-1, 3, 224, 224)  # Reshape output to image dimensions\n\n# Discriminator model with BatchNorm\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.fc1 = nn.Linear(3*224*224, 1024)\n        self.bn1 = nn.BatchNorm1d(1024)\n        self.fc2 = nn.Linear(1024, 512)\n        self.bn2 = nn.BatchNorm1d(512)\n        self.fc3 = nn.Linear(512, 256)\n        self.bn3 = nn.BatchNorm1d(256)\n        self.fc4 = nn.Linear(256, 1)  # Output: 1 for real/fake classification\n\n    def forward(self, x):\n        x = x.view(x.size(0), -1)  # Flatten the image\n        x = F.leaky_relu(self.bn1(self.fc1(x)), 0.2)\n        x = F.leaky_relu(self.bn2(self.fc2(x)), 0.2)\n        x = F.leaky_relu(self.bn3(self.fc3(x)), 0.2)\n        x = torch.sigmoid(self.fc4(x))  # Sigmoid to output probability\n        return x\n\n# Initialize models\ngenerator = Generator().to(device)\ndiscriminator = Discriminator().to(device)\n\n# Noise vector for generator input\ndef generate_noise(batch_size, noise_dim=100):\n    return torch.randn(batch_size, noise_dim, device=device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T12:01:57.335693Z","iopub.execute_input":"2025-01-04T12:01:57.336458Z","iopub.status.idle":"2025-01-04T12:02:00.693129Z","shell.execute_reply.started":"2025-01-04T12:01:57.336419Z","shell.execute_reply":"2025-01-04T12:02:00.692424Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Loss function and optimizers\ncriterion = nn.BCELoss()  # Binary Cross Entropy loss\noptimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002 , betas=(0.5, 0.999))\noptimizer_g = optim.Adam(generator.parameters(), lr=0.00005, betas=(0.5, 0.999))  \n\n# Train DCGAN function\ndef train_dcgan(generator, discriminator, train_loader, epochs):\n    generator.to(device)\n    discriminator.to(device)\n\n    discriminator_losses = []\n    generator_losses = []\n\n    for epoch in range(epochs):\n        epoch_d_loss = 0\n        epoch_g_loss = 0\n\n        for i, (real_images, _) in enumerate(train_loader):\n            real_images = real_images.to(device)\n            batch_size = real_images.size(0)\n\n            # Train Discriminator\n            optimizer_d.zero_grad()\n\n            # Real images with label smoothing (slightly less than 1)\n            label_real = torch.ones(batch_size, 1, device=device) * 0.9\n            output_real = discriminator(real_images)\n            loss_real = criterion(output_real, label_real)\n\n            # Fake images\n            noise = generate_noise(batch_size).to(device)\n            fake_images = generator(noise)\n            label_fake = torch.zeros(batch_size, 1, device=device) * 0.1  # Label smoothing for fake images\n            output_fake = discriminator(fake_images.detach())\n            loss_fake = criterion(output_fake, label_fake)\n\n            # Total discriminator loss\n            loss_d = (loss_real + loss_fake) / 2\n            loss_d.backward()\n            optimizer_d.step()\n\n            # Train Generator\n            optimizer_g.zero_grad()\n\n            output_fake = discriminator(fake_images)  # Fresh fake images\n            loss_g = criterion(output_fake, label_real)  # Generator wants fake to be classified as real\n\n            loss_g.backward()\n            optimizer_g.step()\n\n            # Gradient clipping\n            torch.nn.utils.clip_grad_norm_(discriminator.parameters(), max_norm=1.0)\n            torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)\n\n            epoch_d_loss += loss_d.item()\n            epoch_g_loss += loss_g.item()\n\n        # Print average losses for the epoch\n        avg_d_loss = epoch_d_loss / len(train_loader)\n        avg_g_loss = epoch_g_loss / len(train_loader)\n        print(f\"Epoch [{epoch+1}/{epochs}], D Loss: {avg_d_loss}, G Loss: {avg_g_loss}\")\n\n        # Save average losses for the epoch\n        discriminator_losses.append(avg_d_loss)\n        generator_losses.append(avg_g_loss)\n\n    # Save losses to CSV after all epochs\n    with open('losses.csv', mode='w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Epoch', 'Discriminator Loss', 'Generator Loss'])\n        for epoch in range(epochs):\n            writer.writerow([epoch + 1, discriminator_losses[epoch], generator_losses[epoch]])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T12:02:14.687448Z","iopub.execute_input":"2025-01-04T12:02:14.687782Z","iopub.status.idle":"2025-01-04T12:02:14.699505Z","shell.execute_reply.started":"2025-01-04T12:02:14.687751Z","shell.execute_reply":"2025-01-04T12:02:14.698654Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass FilteredDataset(Dataset):\n    def __init__(self, original_dataset, exclude_class=0):\n        self.filtered_data = [\n            (image, label) for image, label in original_dataset if label != exclude_class\n        ]\n\n    def __len__(self):\n        return len(self.filtered_data)\n\n    def __getitem__(self, idx):\n        return self.filtered_data[idx]\n\n# Filter out class 0\nfiltered_train_dataset = FilteredDataset(train_dataset, exclude_class=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T12:02:18.039219Z","iopub.execute_input":"2025-01-04T12:02:18.039583Z","iopub.status.idle":"2025-01-04T12:07:44.608729Z","shell.execute_reply.started":"2025-01-04T12:02:18.039548Z","shell.execute_reply":"2025-01-04T12:07:44.607747Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Create a new DataLoader with the filtered dataset\ntrain_loader1 = DataLoader(\n    filtered_train_dataset,\n    batch_size=8,\n    shuffle=True,\n    num_workers=4,\n    pin_memory=True,\n    prefetch_factor=2,\n)\ntrain_dcgan(generator, discriminator, train_loader1, epochs=1000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T12:47:07.093772Z","iopub.execute_input":"2025-01-04T12:47:07.094668Z","iopub.status.idle":"2025-01-04T18:19:41.728940Z","shell.execute_reply.started":"2025-01-04T12:47:07.094624Z","shell.execute_reply":"2025-01-04T18:19:41.727824Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/1000], D Loss: 0.2627265112869667, G Loss: 2.3125842130545413\nEpoch [2/1000], D Loss: 0.27069629855228194, G Loss: 2.1846446311835086\nEpoch [3/1000], D Loss: 0.28498949950391594, G Loss: 2.073698303193757\nEpoch [4/1000], D Loss: 0.30242986968069363, G Loss: 1.9313251336415609\nEpoch [5/1000], D Loss: 0.32172522400364734, G Loss: 1.8314844796151826\nEpoch [6/1000], D Loss: 0.34592426265731, G Loss: 1.6853547710360903\nEpoch [7/1000], D Loss: 0.35393138527870177, G Loss: 1.6399061372785857\nEpoch [8/1000], D Loss: 0.3691532400521365, G Loss: 1.5866627086292613\nEpoch [9/1000], D Loss: 0.3618737917957884, G Loss: 1.5386563405846105\nEpoch [10/1000], D Loss: 0.3845257471005122, G Loss: 1.4671705101475572\nEpoch [11/1000], D Loss: 0.38086593394929713, G Loss: 1.4685262347712662\nEpoch [12/1000], D Loss: 0.38478102151191595, G Loss: 1.433103346824646\nEpoch [13/1000], D Loss: 0.40753069592244695, G Loss: 1.3159139759612806\nEpoch [14/1000], D Loss: 0.42068393862608705, G Loss: 1.2974163767063256\nEpoch [15/1000], D Loss: 0.42476807977214004, G Loss: 1.267854223829327\nEpoch [16/1000], D Loss: 0.42466602352532473, G Loss: 1.2759689883752303\nEpoch [17/1000], D Loss: 0.4391998406612512, G Loss: 1.2061633623007573\nEpoch [18/1000], D Loss: 0.4301453639160503, G Loss: 1.2369638883706295\nEpoch [19/1000], D Loss: 0.45423340661959216, G Loss: 1.1444575266404586\nEpoch [20/1000], D Loss: 0.4573788492968588, G Loss: 1.1372354178717643\nEpoch [21/1000], D Loss: 0.48954881023276936, G Loss: 1.0255191893288584\nEpoch [22/1000], D Loss: 0.5101809976678906, G Loss: 0.9517433285713196\nEpoch [23/1000], D Loss: 0.49737515837857216, G Loss: 1.0142102432973457\nEpoch [24/1000], D Loss: 0.5016152661858183, G Loss: 0.9786297454978481\nEpoch [25/1000], D Loss: 0.5200771003058462, G Loss: 0.9399701779538935\nEpoch [26/1000], D Loss: 0.5185532503055804, G Loss: 0.9507929794716112\nEpoch [27/1000], D Loss: 0.5387296707341165, G Loss: 0.8968407099897211\nEpoch [28/1000], D Loss: 0.552604981805339, G Loss: 0.8487182902567314\nEpoch [29/1000], D Loss: 0.5519284484964428, G Loss: 0.8866004566351573\nEpoch [30/1000], D Loss: 0.577080142136776, G Loss: 0.8231721798578898\nEpoch [31/1000], D Loss: 0.5708052710150228, G Loss: 0.8581725899017218\nEpoch [32/1000], D Loss: 0.5944170487649513, G Loss: 0.7787032116543163\nEpoch [33/1000], D Loss: 0.6124266338167769, G Loss: 0.7550246253158107\nEpoch [34/1000], D Loss: 0.6275418758392334, G Loss: 0.7258142194964669\nEpoch [35/1000], D Loss: 0.6157089334545713, G Loss: 0.7568410463405378\nEpoch [36/1000], D Loss: 0.6388620354912498, G Loss: 0.7328842372605295\nEpoch [37/1000], D Loss: 0.6637963197448037, G Loss: 0.6737501153440186\nEpoch [38/1000], D Loss: 0.6937399040568959, G Loss: 0.6515307905095996\nEpoch [39/1000], D Loss: 0.7110964356949835, G Loss: 0.6639049837083527\nEpoch [40/1000], D Loss: 0.7224568392291214, G Loss: 0.6304142294508038\nEpoch [41/1000], D Loss: 0.7365649618885733, G Loss: 0.6235294618389823\nEpoch [42/1000], D Loss: 0.7552346038095878, G Loss: 0.6104265742229693\nEpoch [43/1000], D Loss: 0.7991893842364802, G Loss: 0.5486319196946693\nEpoch [44/1000], D Loss: 0.7945535619150509, G Loss: 0.5757627920670942\nEpoch [45/1000], D Loss: 0.8075710583816875, G Loss: 0.5494330218344023\nEpoch [46/1000], D Loss: 0.7990274832104192, G Loss: 0.5716857142520674\nEpoch [47/1000], D Loss: 0.8287151815313282, G Loss: 0.5406891486861489\nEpoch [48/1000], D Loss: 0.8472475557616262, G Loss: 0.5360185567176703\nEpoch [49/1000], D Loss: 0.8266610326188983, G Loss: 0.5465184860157244\nEpoch [50/1000], D Loss: 0.8759921048626755, G Loss: 0.5165270072041136\nEpoch [51/1000], D Loss: 0.8619250743678122, G Loss: 0.5087898454882882\nEpoch [52/1000], D Loss: 0.8714135943037091, G Loss: 0.5039933504480304\nEpoch [53/1000], D Loss: 0.8750708160978375, G Loss: 0.51010787450906\nEpoch [54/1000], D Loss: 0.8804152819243345, G Loss: 0.5326608347170281\nEpoch [55/1000], D Loss: 0.884271543676203, G Loss: 0.5133851916501017\nEpoch [56/1000], D Loss: 0.9010902197072, G Loss: 0.49992872458515747\nEpoch [57/1000], D Loss: 0.9297787259925495, G Loss: 0.4626795127536311\nEpoch [58/1000], D Loss: 0.9363807582493985, G Loss: 0.4627072471560854\nEpoch [59/1000], D Loss: 0.8955377304192745, G Loss: 0.49034383495648703\nEpoch [60/1000], D Loss: 0.8992933231772798, G Loss: 0.5042061417391805\nEpoch [61/1000], D Loss: 0.9491840945951866, G Loss: 0.4452933781074755\nEpoch [62/1000], D Loss: 0.9790264436692903, G Loss: 0.4440111004945004\nEpoch [63/1000], D Loss: 0.9441149579756187, G Loss: 0.4647946323409225\nEpoch [64/1000], D Loss: 0.9550850779721232, G Loss: 0.4615447243054708\nEpoch [65/1000], D Loss: 0.9469381469668764, G Loss: 0.45876157626961217\nEpoch [66/1000], D Loss: 0.9851289944215255, G Loss: 0.4308743670131221\nEpoch [67/1000], D Loss: 0.968421174179424, G Loss: 0.46334872191602533\nEpoch [68/1000], D Loss: 0.9168406309503497, G Loss: 0.4906760961720438\nEpoch [69/1000], D Loss: 0.9664852512605262, G Loss: 0.45230388514923325\nEpoch [70/1000], D Loss: 0.945622140711004, G Loss: 0.4687415254838539\nEpoch [71/1000], D Loss: 0.9859089865829005, G Loss: 0.4441984938852715\nEpoch [72/1000], D Loss: 1.0215363679510174, G Loss: 0.42687729488719595\nEpoch [73/1000], D Loss: 1.0019434806072351, G Loss: 0.4419803194927447\nEpoch [74/1000], D Loss: 1.0465347510395628, G Loss: 0.4095135865789471\nEpoch [75/1000], D Loss: 1.0546646613063235, G Loss: 0.40996123660694467\nEpoch [76/1000], D Loss: 1.017481157093337, G Loss: 0.4245711888327743\nEpoch [77/1000], D Loss: 1.0424200841874787, G Loss: 0.4162436292026982\nEpoch [78/1000], D Loss: 0.988934685605945, G Loss: 0.44531640247865156\nEpoch [79/1000], D Loss: 1.0284177884911045, G Loss: 0.4249707194891843\nEpoch [80/1000], D Loss: 1.0239181182601236, G Loss: 0.42747007138801346\nEpoch [81/1000], D Loss: 1.025673725749507, G Loss: 0.4256131233591022\nEpoch [82/1000], D Loss: 1.045472892667308, G Loss: 0.42029919118592235\nEpoch [83/1000], D Loss: 1.0509133281129779, G Loss: 0.4264786812392148\nEpoch [84/1000], D Loss: 1.0576927582422893, G Loss: 0.4068949468208082\nEpoch [85/1000], D Loss: 1.042648973609462, G Loss: 0.43769408771485996\nEpoch [86/1000], D Loss: 1.0831946421753276, G Loss: 0.4158437810160897\nEpoch [87/1000], D Loss: 1.0797273081360441, G Loss: 0.40559034781022507\nEpoch [88/1000], D Loss: 1.0586391638625752, G Loss: 0.4355835589495572\nEpoch [89/1000], D Loss: 1.0966104274446313, G Loss: 0.3969990860332142\nEpoch [90/1000], D Loss: 1.1171381975665238, G Loss: 0.38646464293653315\nEpoch [91/1000], D Loss: 1.094583979881171, G Loss: 0.3961563625118949\nEpoch [92/1000], D Loss: 1.0811890641848245, G Loss: 0.4060573306950656\nEpoch [93/1000], D Loss: 1.061966771067995, G Loss: 0.41834013732996855\nEpoch [94/1000], D Loss: 1.055025933005593, G Loss: 0.4203994555906816\nEpoch [95/1000], D Loss: 1.1026228583220279, G Loss: 0.3917123223796035\nEpoch [96/1000], D Loss: 1.0929524291645396, G Loss: 0.40054014740568217\nEpoch [97/1000], D Loss: 1.1257131067189303, G Loss: 0.38258811127055775\nEpoch [98/1000], D Loss: 1.094711468075261, G Loss: 0.4025068091623711\nEpoch [99/1000], D Loss: 1.0874770789435415, G Loss: 0.39160499175389607\nEpoch [100/1000], D Loss: 1.1053841923222398, G Loss: 0.38831817417433767\nEpoch [101/1000], D Loss: 1.0985285314646633, G Loss: 0.3940897378054532\nEpoch [102/1000], D Loss: 1.1383041932727351, G Loss: 0.38260310978600476\nEpoch [103/1000], D Loss: 1.1806386969306253, G Loss: 0.3660735948519273\nEpoch [104/1000], D Loss: 1.1602634411869628, G Loss: 0.3853340244654453\nEpoch [105/1000], D Loss: 1.1393051028251648, G Loss: 0.3895620281046087\nEpoch [106/1000], D Loss: 1.1154285223195046, G Loss: 0.3991774902199254\nEpoch [107/1000], D Loss: 1.129934503815391, G Loss: 0.3895374561801101\nEpoch [108/1000], D Loss: 1.1502046046835004, G Loss: 0.3838408159487175\nEpoch [109/1000], D Loss: 1.1595658685221817, G Loss: 0.37666142293901156\nEpoch [110/1000], D Loss: 1.1643402543934909, G Loss: 0.37323911370653096\nEpoch [111/1000], D Loss: 1.1782724431066802, G Loss: 0.3732093252918937\nEpoch [112/1000], D Loss: 1.1344904867085543, G Loss: 0.38977301662618463\nEpoch [113/1000], D Loss: 1.1326559782028198, G Loss: 0.38445489569143815\nEpoch [114/1000], D Loss: 1.0758326255913937, G Loss: 0.42180946108066675\nEpoch [115/1000], D Loss: 1.1215329708475055, G Loss: 0.4003753476070635\nEpoch [116/1000], D Loss: 1.16941517013492, G Loss: 0.3763960153767557\nEpoch [117/1000], D Loss: 1.1622700329982874, G Loss: 0.3742024871436032\nEpoch [118/1000], D Loss: 1.1899154793132436, G Loss: 0.35916857556863263\nEpoch [119/1000], D Loss: 1.2003069231004426, G Loss: 0.3617665045189135\nEpoch [120/1000], D Loss: 1.156955160516681, G Loss: 0.38879303480639604\nEpoch [121/1000], D Loss: 1.1482225273594713, G Loss: 0.37666586926489165\nEpoch [122/1000], D Loss: 1.1495363011504665, G Loss: 0.38653482234839237\nEpoch [123/1000], D Loss: 1.168831734043179, G Loss: 0.37860894112875965\nEpoch [124/1000], D Loss: 1.1485988082307759, G Loss: 0.38314755522843563\nEpoch [125/1000], D Loss: 1.1937772967598654, G Loss: 0.36469708518548444\nEpoch [126/1000], D Loss: 1.1972092169703858, G Loss: 0.37307355060721886\nEpoch [127/1000], D Loss: 1.198034817160982, G Loss: 0.3666598018371698\nEpoch [128/1000], D Loss: 1.1925037989110658, G Loss: 0.3688951436317328\nEpoch [129/1000], D Loss: 1.1640350948680531, G Loss: 0.3703466222141728\nEpoch [130/1000], D Loss: 1.1825696172136249, G Loss: 0.376459198106419\nEpoch [131/1000], D Loss: 1.1085507996154553, G Loss: 0.40602741747191456\nEpoch [132/1000], D Loss: 1.1674396960121212, G Loss: 0.3855002735600327\nEpoch [133/1000], D Loss: 1.1473035696781042, G Loss: 0.3828594522042708\nEpoch [134/1000], D Loss: 1.1975300106135283, G Loss: 0.3631586815371658\nEpoch [135/1000], D Loss: 1.18177840312322, G Loss: 0.3697717567284902\nEpoch [136/1000], D Loss: 1.1790046991723957, G Loss: 0.3817594387314536\nEpoch [137/1000], D Loss: 1.1820642604972378, G Loss: 0.3654629976460428\nEpoch [138/1000], D Loss: 1.2283688971490572, G Loss: 0.35344885659940317\nEpoch [139/1000], D Loss: 1.1958281354470686, G Loss: 0.3665715737776323\nEpoch [140/1000], D Loss: 1.1823059923721082, G Loss: 0.3673024600202387\nEpoch [141/1000], D Loss: 1.2184790257251623, G Loss: 0.3571356444647818\nEpoch [142/1000], D Loss: 1.1894193181485841, G Loss: 0.37046683701601896\nEpoch [143/1000], D Loss: 1.1581316232681274, G Loss: 0.3889492882020546\nEpoch [144/1000], D Loss: 1.1601949254671733, G Loss: 0.37092242800828185\nEpoch [145/1000], D Loss: 1.2191357005726207, G Loss: 0.3549121247999596\nEpoch [146/1000], D Loss: 1.2103610428896818, G Loss: 0.3600203786835526\nEpoch [147/1000], D Loss: 1.1958905281442584, G Loss: 0.36432902271097356\nEpoch [148/1000], D Loss: 1.1659688400499748, G Loss: 0.3776836987697717\nEpoch [149/1000], D Loss: 1.221519361481522, G Loss: 0.35247857715144304\nEpoch [150/1000], D Loss: 1.1927054064743448, G Loss: 0.3831085508519953\nEpoch [151/1000], D Loss: 1.1845016497554202, G Loss: 0.3730431110569925\nEpoch [152/1000], D Loss: 1.231248869679191, G Loss: 0.35347889372796726\nEpoch [153/1000], D Loss: 1.2494954257300406, G Loss: 0.3469766419945341\nEpoch [154/1000], D Loss: 1.189355360919779, G Loss: 0.3691311270901651\nEpoch [155/1000], D Loss: 1.2014662692041107, G Loss: 0.3704939573100119\nEpoch [156/1000], D Loss: 1.2277620564807545, G Loss: 0.3566755083474246\nEpoch [157/1000], D Loss: 1.2223016110333529, G Loss: 0.3646573411695885\nEpoch [158/1000], D Loss: 1.209838911258813, G Loss: 0.35913141622687833\nEpoch [159/1000], D Loss: 1.1991467900348431, G Loss: 0.37595462654576156\nEpoch [160/1000], D Loss: 1.239416630520965, G Loss: 0.3644473321510084\nEpoch [161/1000], D Loss: 1.2470189166791512, G Loss: 0.35350423437176326\nEpoch [162/1000], D Loss: 1.2055673881010576, G Loss: 0.3687393667119922\nEpoch [163/1000], D Loss: 1.2767863880504262, G Loss: 0.34944534175323716\nEpoch [164/1000], D Loss: 1.2138020916418595, G Loss: 0.3590178179018425\nEpoch [165/1000], D Loss: 1.185904595526782, G Loss: 0.36713222536173734\nEpoch [166/1000], D Loss: 1.2541481205911347, G Loss: 0.3536625679695245\nEpoch [167/1000], D Loss: 1.2017120336041307, G Loss: 0.37229928627158654\nEpoch [168/1000], D Loss: 1.2394215085289695, G Loss: 0.35239003734155133\nEpoch [169/1000], D Loss: 1.2585677139686815, G Loss: 0.35128886410684296\nEpoch [170/1000], D Loss: 1.2301962973493519, G Loss: 0.35715141802123096\nEpoch [171/1000], D Loss: 1.2179757490302576, G Loss: 0.3706858342344111\nEpoch [172/1000], D Loss: 1.1980718244205821, G Loss: 0.37525175629240093\nEpoch [173/1000], D Loss: 1.2523339372692686, G Loss: 0.3562785657969388\nEpoch [174/1000], D Loss: 1.2553087259783888, G Loss: 0.353976103031274\nEpoch [175/1000], D Loss: 1.2480094049916124, G Loss: 0.3581120091857332\nEpoch [176/1000], D Loss: 1.2320750297922076, G Loss: 0.36618763930869824\nEpoch [177/1000], D Loss: 1.2268543489051589, G Loss: 0.37692363533106715\nEpoch [178/1000], D Loss: 1.2325213316715125, G Loss: 0.3562261563358885\nEpoch [179/1000], D Loss: 1.2528645826108529, G Loss: 0.35001068946087\nEpoch [180/1000], D Loss: 1.2377243757247924, G Loss: 0.36170226082657325\nEpoch [181/1000], D Loss: 1.2239231745402017, G Loss: 0.36726615609544694\nEpoch [182/1000], D Loss: 1.2582307327877391, G Loss: 0.35420482718583307\nEpoch [183/1000], D Loss: 1.2468957059311143, G Loss: 0.3537000072724891\nEpoch [184/1000], D Loss: 1.2267518729874582, G Loss: 0.36676965193314987\nEpoch [185/1000], D Loss: 1.2700552727236891, G Loss: 0.3533372478051619\nEpoch [186/1000], D Loss: 1.262854637521686, G Loss: 0.3529096001928503\nEpoch [187/1000], D Loss: 1.197671108534842, G Loss: 0.37338143225872156\nEpoch [188/1000], D Loss: 1.2377495819872075, G Loss: 0.3663019046638951\nEpoch [189/1000], D Loss: 1.2777875427043799, G Loss: 0.34711546030911533\nEpoch [190/1000], D Loss: 1.2970797914447207, G Loss: 0.3454739720532388\nEpoch [191/1000], D Loss: 1.2239558274095708, G Loss: 0.37288052114573395\nEpoch [192/1000], D Loss: 1.2215311801794804, G Loss: 0.3718470443378795\nEpoch [193/1000], D Loss: 1.245149342399655, G Loss: 0.35879960999344335\nEpoch [194/1000], D Loss: 1.2705084515340401, G Loss: 0.34752424666375825\nEpoch [195/1000], D Loss: 1.2847772728313098, G Loss: 0.34684555837602327\nEpoch [196/1000], D Loss: 1.2441882035949015, G Loss: 0.36325764240640585\nEpoch [197/1000], D Loss: 1.2386587529471427, G Loss: 0.3608259947010965\nEpoch [198/1000], D Loss: 1.260869177904996, G Loss: 0.353263305534016\nEpoch [199/1000], D Loss: 1.2401383515560265, G Loss: 0.35639202359950906\nEpoch [200/1000], D Loss: 1.2668944864562064, G Loss: 0.3517969398787527\nEpoch [201/1000], D Loss: 1.2151890826947762, G Loss: 0.37554085579785434\nEpoch [202/1000], D Loss: 1.244953587922183, G Loss: 0.3591317695198637\nEpoch [203/1000], D Loss: 1.2710345293536331, G Loss: 0.3479422224290443\nEpoch [204/1000], D Loss: 1.2914217500975638, G Loss: 0.3445722018227433\nEpoch [205/1000], D Loss: 1.295157383066235, G Loss: 0.3448536302104141\nEpoch [206/1000], D Loss: 1.2732652768944248, G Loss: 0.3504817767576738\nEpoch [207/1000], D Loss: 1.266968090245218, G Loss: 0.3485729502909111\nEpoch [208/1000], D Loss: 1.26937845584118, G Loss: 0.34665171937509015\nEpoch [209/1000], D Loss: 1.2780828168897918, G Loss: 0.3445675597046361\nEpoch [210/1000], D Loss: 1.2857102968476035, G Loss: 0.34588109038092874\nEpoch [211/1000], D Loss: 1.255876036123796, G Loss: 0.350455229029511\nEpoch [212/1000], D Loss: 1.263971508994247, G Loss: 0.3501685565168207\nEpoch [213/1000], D Loss: 1.2738897135763458, G Loss: 0.3473471189990188\nEpoch [214/1000], D Loss: 1.2817395654591648, G Loss: 0.3469337486859524\nEpoch [215/1000], D Loss: 1.2570403258005778, G Loss: 0.35597787553613836\nEpoch [216/1000], D Loss: 1.2667445096102627, G Loss: 0.34908349839123815\nEpoch [217/1000], D Loss: 1.2440670822605941, G Loss: 0.35338796630050195\nEpoch [218/1000], D Loss: 1.27953619667978, G Loss: 0.34182915272134723\nEpoch [219/1000], D Loss: 1.2692757639017973, G Loss: 0.34991280165585603\nEpoch [220/1000], D Loss: 1.2711100368788748, G Loss: 0.3551741659641266\nEpoch [221/1000], D Loss: 1.2782924048828357, G Loss: 0.3492330276604855\nEpoch [222/1000], D Loss: 1.2524132519057303, G Loss: 0.36287343772974884\nEpoch [223/1000], D Loss: 1.2754055099053816, G Loss: 0.3421629797328602\nEpoch [224/1000], D Loss: 1.2881072875225184, G Loss: 0.3404322902361552\nEpoch [225/1000], D Loss: 1.2780351851925706, G Loss: 0.3475661328344634\nEpoch [226/1000], D Loss: 1.283720950646834, G Loss: 0.34557875575441305\nEpoch [227/1000], D Loss: 1.258498228860624, G Loss: 0.35285621718926863\nEpoch [228/1000], D Loss: 1.2491732163862748, G Loss: 0.3560673811218955\nEpoch [229/1000], D Loss: 1.268092728022373, G Loss: 0.3544362405935923\nEpoch [230/1000], D Loss: 1.2399493909243382, G Loss: 0.3572059441696514\nEpoch [231/1000], D Loss: 1.2955159136743257, G Loss: 0.3434050527485934\nEpoch [232/1000], D Loss: 1.2892238967346423, G Loss: 0.3447851764433312\nEpoch [233/1000], D Loss: 1.2978330471298911, G Loss: 0.3434775565609787\nEpoch [234/1000], D Loss: 1.3027637488914259, G Loss: 0.33797745831084974\nEpoch [235/1000], D Loss: 1.2832444032033286, G Loss: 0.3468277526624275\nEpoch [236/1000], D Loss: 1.289004185705474, G Loss: 0.3423563296144659\nEpoch [237/1000], D Loss: 1.2818116654049267, G Loss: 0.3475212646253181\nEpoch [238/1000], D Loss: 1.2720579638625635, G Loss: 0.34565150123653987\nEpoch [239/1000], D Loss: 1.2760798606005581, G Loss: 0.3490527633464698\nEpoch [240/1000], D Loss: 1.2637778238816695, G Loss: 0.3482153224222588\nEpoch [241/1000], D Loss: 1.2744915369785192, G Loss: 0.34764498761205964\nEpoch [242/1000], D Loss: 1.2952070554097495, G Loss: 0.34318278287396287\nEpoch [243/1000], D Loss: 1.3190520604451497, G Loss: 0.3344175143675371\nEpoch [244/1000], D Loss: 1.2665691527453335, G Loss: 0.3416201508406437\nEpoch [245/1000], D Loss: 1.262317910338893, G Loss: 0.3524335425911528\nEpoch [246/1000], D Loss: 1.2516258348118174, G Loss: 0.37243159413337706\nEpoch [247/1000], D Loss: 1.2886297166347505, G Loss: 0.346609510978063\nEpoch [248/1000], D Loss: 1.294201613556255, G Loss: 0.3395612369884144\nEpoch [249/1000], D Loss: 1.26095894557057, G Loss: 0.3631824809493441\nEpoch [250/1000], D Loss: 1.2974268981904695, G Loss: 0.34371444366194986\nEpoch [251/1000], D Loss: 1.315074488249692, G Loss: 0.3410072006962516\nEpoch [252/1000], D Loss: 1.2830873893968986, G Loss: 0.34396106347893224\nEpoch [253/1000], D Loss: 1.271852228497014, G Loss: 0.3446292792305802\nEpoch [254/1000], D Loss: 1.303037444750468, G Loss: 0.3425902023459926\nEpoch [255/1000], D Loss: 1.2916359027226767, G Loss: 0.34268279906475185\nEpoch [256/1000], D Loss: 1.2972658464402864, G Loss: 0.3424986732728554\nEpoch [257/1000], D Loss: 1.3014527356985843, G Loss: 0.3388388953425667\nEpoch [258/1000], D Loss: 1.2838475758379155, G Loss: 0.34763687379432445\nEpoch [259/1000], D Loss: 1.2835671916152491, G Loss: 0.3466883662975196\nEpoch [260/1000], D Loss: 1.2983175747322313, G Loss: 0.3401701549688975\nEpoch [261/1000], D Loss: 1.2988643671527054, G Loss: 0.34389482339223226\nEpoch [262/1000], D Loss: 1.2992110920674873, G Loss: 0.35259445454135085\nEpoch [263/1000], D Loss: 1.2469360647779522, G Loss: 0.3574906997608416\nEpoch [264/1000], D Loss: 1.2772920001636852, G Loss: 0.34489731391270956\nEpoch [265/1000], D Loss: 1.2928292310599125, G Loss: 0.33760613546226964\nEpoch [266/1000], D Loss: 1.280435146707477, G Loss: 0.3468534809170347\nEpoch [267/1000], D Loss: 1.2733389648524198, G Loss: 0.3536797630064415\nEpoch [268/1000], D Loss: 1.287290968316974, G Loss: 0.3425679815538002\nEpoch [269/1000], D Loss: 1.2390228668848673, G Loss: 0.35970351009657886\nEpoch [270/1000], D Loss: 1.2828390182870808, G Loss: 0.3453966390002858\nEpoch [271/1000], D Loss: 1.2616623556975162, G Loss: 0.3485207227143374\nEpoch [272/1000], D Loss: 1.3075236497503337, G Loss: 0.33803634011384215\nEpoch [273/1000], D Loss: 1.2922617984540534, G Loss: 0.3419782221317291\nEpoch [274/1000], D Loss: 1.3084041013862147, G Loss: 0.3368689038536765\nEpoch [275/1000], D Loss: 1.2803553487315322, G Loss: 0.3452647359082193\nEpoch [276/1000], D Loss: 1.2464824205095117, G Loss: 0.36795312708074396\nEpoch [277/1000], D Loss: 1.3005955443237767, G Loss: 0.3408609386646386\nEpoch [278/1000], D Loss: 1.2991598320729805, G Loss: 0.34115392330921057\nEpoch [279/1000], D Loss: 1.2990510763544025, G Loss: 0.34024340745174525\nEpoch [280/1000], D Loss: 1.3024093754363784, G Loss: 0.3392404005383\nEpoch [281/1000], D Loss: 1.2949213201349432, G Loss: 0.34542910402471366\nEpoch [282/1000], D Loss: 1.3067616733637724, G Loss: 0.3385515131733634\nEpoch [283/1000], D Loss: 1.3036000096436702, G Loss: 0.3385671109864206\nEpoch [284/1000], D Loss: 1.3050588044253262, G Loss: 0.33728500640753545\nEpoch [285/1000], D Loss: 1.2903237187501155, G Loss: 0.339299148682392\nEpoch [286/1000], D Loss: 1.3136783422845784, G Loss: 0.3390873778950084\nEpoch [287/1000], D Loss: 1.282569222016768, G Loss: 0.3415112452073531\nEpoch [288/1000], D Loss: 1.290141782616124, G Loss: 0.341618254329219\nEpoch [289/1000], D Loss: 1.2721500617085082, G Loss: 0.3507906810803847\nEpoch [290/1000], D Loss: 1.3055158174399173, G Loss: 0.3391155925664035\nEpoch [291/1000], D Loss: 1.2825376642472817, G Loss: 0.34279076475085635\nEpoch [292/1000], D Loss: 1.2901750535675973, G Loss: 0.34027294263695224\nEpoch [293/1000], D Loss: 1.293974513718576, G Loss: 0.34086242697455665\nEpoch [294/1000], D Loss: 1.2706494322328856, G Loss: 0.3519748158527143\nEpoch [295/1000], D Loss: 1.2739994034622655, G Loss: 0.3467943216815139\nEpoch [296/1000], D Loss: 1.2766508788773507, G Loss: 0.34680458000212006\nEpoch [297/1000], D Loss: 1.2979584397691668, G Loss: 0.34219141548330134\nEpoch [298/1000], D Loss: 1.2713405273177407, G Loss: 0.3589192374186082\nEpoch [299/1000], D Loss: 1.2968758550557222, G Loss: 0.340307260462732\nEpoch [300/1000], D Loss: 1.3070849671508327, G Loss: 0.33843750989798344\nEpoch [301/1000], D Loss: 1.3212188789338777, G Loss: 0.3371835574959264\nEpoch [302/1000], D Loss: 1.2783030697793671, G Loss: 0.3414139418890982\nEpoch [303/1000], D Loss: 1.3173474019223994, G Loss: 0.33511142405596644\nEpoch [304/1000], D Loss: 1.3117532708428123, G Loss: 0.3401816472862706\nEpoch [305/1000], D Loss: 1.2969104611512385, G Loss: 0.3397093881260265\nEpoch [306/1000], D Loss: 1.2979601888945609, G Loss: 0.3435917279937051\nEpoch [307/1000], D Loss: 1.29936893528158, G Loss: 0.33924796093593945\nEpoch [308/1000], D Loss: 1.2681652668750647, G Loss: 0.342985435146274\nEpoch [309/1000], D Loss: 1.295487402785908, G Loss: 0.34215676459399136\nEpoch [310/1000], D Loss: 1.2521885203592704, G Loss: 0.35011066144162956\nEpoch [311/1000], D Loss: 1.2314362374219028, G Loss: 0.3572723666826884\nEpoch [312/1000], D Loss: 1.2933031790184253, G Loss: 0.3387780231056791\nEpoch [313/1000], D Loss: 1.2887287660078568, G Loss: 0.33942276112961045\nEpoch [314/1000], D Loss: 1.2708277369990493, G Loss: 0.3502779436833931\nEpoch [315/1000], D Loss: 1.2711099147796632, G Loss: 0.34833138729586743\nEpoch [316/1000], D Loss: 1.2881092486959516, G Loss: 0.34317690603660816\nEpoch [317/1000], D Loss: 1.2768784010049068, G Loss: 0.3424412205363765\nEpoch [318/1000], D Loss: 1.304058134917057, G Loss: 0.33574325081073875\nEpoch [319/1000], D Loss: 1.304880118189436, G Loss: 0.3406869574026628\nEpoch [320/1000], D Loss: 1.309538527691003, G Loss: 0.3395845814184709\nEpoch [321/1000], D Loss: 1.3064776547027357, G Loss: 0.336914825258833\nEpoch [322/1000], D Loss: 1.3048791054523352, G Loss: 0.3415219086589235\nEpoch [323/1000], D Loss: 1.2966335061824683, G Loss: 0.3476234333081679\nEpoch [324/1000], D Loss: 1.3057380437850952, G Loss: 0.33713387323148325\nEpoch [325/1000], D Loss: 1.3003710096532648, G Loss: 0.33630626346125747\nEpoch [326/1000], D Loss: 1.2878348341493895, G Loss: 0.34067842129505044\nEpoch [327/1000], D Loss: 1.2731451374111753, G Loss: 0.34995661504340897\nEpoch [328/1000], D Loss: 1.2640559973138752, G Loss: 0.3541902959346771\nEpoch [329/1000], D Loss: 1.2753885540095242, G Loss: 0.34553175044782236\nEpoch [330/1000], D Loss: 1.2769453554442434, G Loss: 0.347804277232199\nEpoch [331/1000], D Loss: 1.289553986896168, G Loss: 0.3448619636622342\nEpoch [332/1000], D Loss: 1.2985807068420179, G Loss: 0.34643373399069816\nEpoch [333/1000], D Loss: 1.308978633808367, G Loss: 0.33661311792604853\nEpoch [334/1000], D Loss: 1.3150475827130403, G Loss: 0.33725770332596516\nEpoch [335/1000], D Loss: 1.3188640637831255, G Loss: 0.33235804944327385\nEpoch [336/1000], D Loss: 1.3220601193832628, G Loss: 0.3340043857242122\nEpoch [337/1000], D Loss: 1.3032984173659123, G Loss: 0.33528055530605894\nEpoch [338/1000], D Loss: 1.2833548025651411, G Loss: 0.3407112029465762\nEpoch [339/1000], D Loss: 1.2925014127384533, G Loss: 0.34526894489924115\nEpoch [340/1000], D Loss: 1.3134254798744665, G Loss: 0.3352550389188709\nEpoch [341/1000], D Loss: 1.3026198748386268, G Loss: 0.3411055554043163\nEpoch [342/1000], D Loss: 1.3133126096291976, G Loss: 0.3399891598658128\nEpoch [343/1000], D Loss: 1.3041736563046773, G Loss: 0.34429741393436086\nEpoch [344/1000], D Loss: 1.3192435318773443, G Loss: 0.33479049856012516\nEpoch [345/1000], D Loss: 1.3011622580614957, G Loss: 0.33781651619708897\nEpoch [346/1000], D Loss: 1.3166670058712815, G Loss: 0.3362115690202424\nEpoch [347/1000], D Loss: 1.3088992443951692, G Loss: 0.3365412260546829\nEpoch [348/1000], D Loss: 1.2840729745951567, G Loss: 0.3476690328482426\nEpoch [349/1000], D Loss: 1.3034139336961688, G Loss: 0.3397434940843871\nEpoch [350/1000], D Loss: 1.3033492532643405, G Loss: 0.3396871628183307\nEpoch [351/1000], D Loss: 1.2579218687433185, G Loss: 0.34828868273532754\nEpoch [352/1000], D Loss: 1.3075553807345304, G Loss: 0.3353329113035491\nEpoch [353/1000], D Loss: 1.297334763136777, G Loss: 0.33990041162028456\nEpoch [354/1000], D Loss: 1.307300991000551, G Loss: 0.3394483102090431\nEpoch [355/1000], D Loss: 1.3014236645265058, G Loss: 0.3375754219112974\nEpoch [356/1000], D Loss: 1.312206496253158, G Loss: 0.33687956423470466\nEpoch [357/1000], D Loss: 1.288193242116408, G Loss: 0.3399178342385726\nEpoch [358/1000], D Loss: 1.264533930836302, G Loss: 0.3523205128583041\nEpoch [359/1000], D Loss: 1.2705407561677875, G Loss: 0.34649332292152174\nEpoch [360/1000], D Loss: 1.3037488359393496, G Loss: 0.3376958937355966\nEpoch [361/1000], D Loss: 1.2863611315235948, G Loss: 0.3389135936896006\nEpoch [362/1000], D Loss: 1.2986042420069377, G Loss: 0.3367769263007424\nEpoch [363/1000], D Loss: 1.3153768741723262, G Loss: 0.3341303673657504\nEpoch [364/1000], D Loss: 1.310863843831149, G Loss: 0.33185861634485647\nEpoch [365/1000], D Loss: 1.2941352504672425, G Loss: 0.3386407574017843\nEpoch [366/1000], D Loss: 1.2481434785958492, G Loss: 0.3602836518576651\nEpoch [367/1000], D Loss: 1.2827223680236124, G Loss: 0.3421554733406414\nEpoch [368/1000], D Loss: 1.294841998273676, G Loss: 0.3489942104527445\nEpoch [369/1000], D Loss: 1.3031544407208762, G Loss: 0.3391431322603515\nEpoch [370/1000], D Loss: 1.3107723929665305, G Loss: 0.33484475739074476\nEpoch [371/1000], D Loss: 1.3008751385139696, G Loss: 0.3340098216678157\nEpoch [372/1000], D Loss: 1.3255832274754842, G Loss: 0.33229030461022346\nEpoch [373/1000], D Loss: 1.3057142770651615, G Loss: 0.3368134424541936\nEpoch [374/1000], D Loss: 1.3110881072102172, G Loss: 0.33791671991348265\nEpoch [375/1000], D Loss: 1.311648568240079, G Loss: 0.33619095881779987\nEpoch [376/1000], D Loss: 1.3130209279782845, G Loss: 0.3362752564025648\nEpoch [377/1000], D Loss: 1.2861826878605467, G Loss: 0.3467496333700238\nEpoch [378/1000], D Loss: 1.309314859274662, G Loss: 0.33758332855773693\nEpoch [379/1000], D Loss: 1.2996806628776318, G Loss: 0.3396488955526641\nEpoch [380/1000], D Loss: 1.2953684055443966, G Loss: 0.33631752783601937\nEpoch [381/1000], D Loss: 1.2853786071141562, G Loss: 0.3421726898713545\nEpoch [382/1000], D Loss: 1.3050679474165945, G Loss: 0.3340449795578465\nEpoch [383/1000], D Loss: 1.3146732359221487, G Loss: 0.33862145163796165\nEpoch [384/1000], D Loss: 1.306708818132227, G Loss: 0.3344851031447902\nEpoch [385/1000], D Loss: 1.2844619740139354, G Loss: 0.34622545170061514\nEpoch [386/1000], D Loss: 1.2886839245304917, G Loss: 0.3462567546150901\nEpoch [387/1000], D Loss: 1.3274891943642588, G Loss: 0.33204927480582036\nEpoch [388/1000], D Loss: 1.317629267952659, G Loss: 0.3341733592929262\nEpoch [389/1000], D Loss: 1.2831082876884576, G Loss: 0.34296257622314225\nEpoch [390/1000], D Loss: 1.3009912342736216, G Loss: 0.34251337936430265\nEpoch [391/1000], D Loss: 1.3091707280187896, G Loss: 0.33762820092114537\nEpoch [392/1000], D Loss: 1.2919047926411484, G Loss: 0.3433775419538671\nEpoch [393/1000], D Loss: 1.293387403271415, G Loss: 0.3368913538528211\nEpoch [394/1000], D Loss: 1.3170272570667845, G Loss: 0.33638860619429384\nEpoch [395/1000], D Loss: 1.303325689200199, G Loss: 0.33603917251933707\nEpoch [396/1000], D Loss: 1.2841883625044967, G Loss: 0.3439882623426842\nEpoch [397/1000], D Loss: 1.2862573334665008, G Loss: 0.33854330651687853\nEpoch [398/1000], D Loss: 1.3180065187540921, G Loss: 0.3372128912896821\nEpoch [399/1000], D Loss: 1.2967547972997029, G Loss: 0.337028551643545\nEpoch [400/1000], D Loss: 1.3138938629265988, G Loss: 0.33325898412502175\nEpoch [401/1000], D Loss: 1.2800416465961573, G Loss: 0.3392520070075989\nEpoch [402/1000], D Loss: 1.323836821859533, G Loss: 0.33424610477505307\nEpoch [403/1000], D Loss: 1.3044496232813054, G Loss: 0.3359223472349571\nEpoch [404/1000], D Loss: 1.2745510281938495, G Loss: 0.3421336809794108\nEpoch [405/1000], D Loss: 1.288120492660638, G Loss: 0.33926955696308253\nEpoch [406/1000], D Loss: 1.3099577047608115, G Loss: 0.33857330138033087\nEpoch [407/1000], D Loss: 1.2776431437694664, G Loss: 0.34479782960631633\nEpoch [408/1000], D Loss: 1.272860528122295, G Loss: 0.3467554327213403\nEpoch [409/1000], D Loss: 1.3045796087293915, G Loss: 0.33549743077971717\nEpoch [410/1000], D Loss: 1.2831519014907606, G Loss: 0.3424439852887934\nEpoch [411/1000], D Loss: 1.282033667419896, G Loss: 0.3339596134243589\nEpoch [412/1000], D Loss: 1.300066960219181, G Loss: 0.3372070868810018\nEpoch [413/1000], D Loss: 1.326652347319054, G Loss: 0.3333855887254079\nEpoch [414/1000], D Loss: 1.2736846573425062, G Loss: 0.3406554227525538\nEpoch [415/1000], D Loss: 1.2712609630642515, G Loss: 0.34064798264792473\nEpoch [416/1000], D Loss: 1.3094041405302106, G Loss: 0.3331710055018916\nEpoch [417/1000], D Loss: 1.325162976438349, G Loss: 0.3336324856136784\nEpoch [418/1000], D Loss: 1.3058535597541117, G Loss: 0.33534948428471884\nEpoch [419/1000], D Loss: 1.2940957506497701, G Loss: 0.3369553515405366\nEpoch [420/1000], D Loss: 1.306046430269877, G Loss: 0.3432661415952625\nEpoch [421/1000], D Loss: 1.3128109469558253, G Loss: 0.33513129100655065\nEpoch [422/1000], D Loss: 1.30629938732494, G Loss: 0.33160190654523447\nEpoch [423/1000], D Loss: 1.3123277418541186, G Loss: 0.33621775923353253\nEpoch [424/1000], D Loss: 1.3170997872497097, G Loss: 0.3322920947363882\nEpoch [425/1000], D Loss: 1.291970357389161, G Loss: 0.34049788695393185\nEpoch [426/1000], D Loss: 1.3116721460313507, G Loss: 0.33844665198615104\nEpoch [427/1000], D Loss: 1.3057085810285627, G Loss: 0.336265274069526\nEpoch [428/1000], D Loss: 1.3087245858076848, G Loss: 0.3384688928271785\nEpoch [429/1000], D Loss: 1.3189850836089163, G Loss: 0.3341535205190832\nEpoch [430/1000], D Loss: 1.2928294481653155, G Loss: 0.3456318983525941\nEpoch [431/1000], D Loss: 1.2909223599867388, G Loss: 0.33477828574903085\nEpoch [432/1000], D Loss: 1.2940320004116406, G Loss: 0.3400062228694107\nEpoch [433/1000], D Loss: 1.2868856950239702, G Loss: 0.33999804854393006\nEpoch [434/1000], D Loss: 1.2754025679646117, G Loss: 0.3397782235434561\nEpoch [435/1000], D Loss: 1.2719804709607905, G Loss: 0.34673795013716724\nEpoch [436/1000], D Loss: 1.3028965158896013, G Loss: 0.33249838478637467\nEpoch [437/1000], D Loss: 1.3063890081463438, G Loss: 0.3354146406506047\nEpoch [438/1000], D Loss: 1.2922815153093048, G Loss: 0.3381471250996445\nEpoch [439/1000], D Loss: 1.2760365381385341, G Loss: 0.34751974037199307\nEpoch [440/1000], D Loss: 1.307983321132082, G Loss: 0.3307478975165974\nEpoch [441/1000], D Loss: 1.2946729768406262, G Loss: 0.3360011734745719\nEpoch [442/1000], D Loss: 1.2991998329307093, G Loss: 0.336116304361459\nEpoch [443/1000], D Loss: 1.2978465112772855, G Loss: 0.3352069078069745\nEpoch [444/1000], D Loss: 1.3027215469967235, G Loss: 0.33438805540402733\nEpoch [445/1000], D Loss: 1.3185859868020722, G Loss: 0.3335456107601975\nEpoch [446/1000], D Loss: 1.305809864130887, G Loss: 0.3377265727881229\nEpoch [447/1000], D Loss: 1.2918221022143508, G Loss: 0.3365990196213578\nEpoch [448/1000], D Loss: 1.3159313079082604, G Loss: 0.33247067205833664\nEpoch [449/1000], D Loss: 1.3299581303740993, G Loss: 0.3297262551206531\nEpoch [450/1000], D Loss: 1.319703722000122, G Loss: 0.329957720005151\nEpoch [451/1000], D Loss: 1.3167220549149947, G Loss: 0.33407549262046815\nEpoch [452/1000], D Loss: 1.3172173153270375, G Loss: 0.3345576895005775\nEpoch [453/1000], D Loss: 1.2828741904461023, G Loss: 0.34826590870365953\nEpoch [454/1000], D Loss: 1.2941251368233653, G Loss: 0.33703530033429463\nEpoch [455/1000], D Loss: 1.3299100561575457, G Loss: 0.3316429212237849\nEpoch [456/1000], D Loss: 1.314013291127754, G Loss: 0.3314015047116713\nEpoch [457/1000], D Loss: 1.275005407405622, G Loss: 0.3476703936403448\nEpoch [458/1000], D Loss: 1.2744646747907002, G Loss: 0.35522723721735405\nEpoch [459/1000], D Loss: 1.3043125788370769, G Loss: 0.3391376374345837\nEpoch [460/1000], D Loss: 1.2961703065669898, G Loss: 0.33750311154307744\nEpoch [461/1000], D Loss: 1.3032275351611051, G Loss: 0.3369620594111356\nEpoch [462/1000], D Loss: 1.3021824110638012, G Loss: 0.3385793017618584\nEpoch [463/1000], D Loss: 1.3153007716843577, G Loss: 0.3311915914217631\nEpoch [464/1000], D Loss: 1.2930652221043906, G Loss: 0.3358758854143547\nEpoch [465/1000], D Loss: 1.324817803773013, G Loss: 0.3319799482822418\nEpoch [466/1000], D Loss: 1.3116232239838803, G Loss: 0.3369370478572267\nEpoch [467/1000], D Loss: 1.318128198927099, G Loss: 0.3342885765162381\nEpoch [468/1000], D Loss: 1.3207186912045334, G Loss: 0.33034690528204946\nEpoch [469/1000], D Loss: 1.3136116916483098, G Loss: 0.33441740996909863\nEpoch [470/1000], D Loss: 1.3053825667410186, G Loss: 0.3322397911187374\nEpoch [471/1000], D Loss: 1.330775058630741, G Loss: 0.32996469609665147\nEpoch [472/1000], D Loss: 1.3125993891195817, G Loss: 0.3331580234296394\nEpoch [473/1000], D Loss: 1.3036347808259907, G Loss: 0.33818996259660433\nEpoch [474/1000], D Loss: 1.3077805428793936, G Loss: 0.33135206934177514\nEpoch [475/1000], D Loss: 1.308219839587356, G Loss: 0.3340224134199547\nEpoch [476/1000], D Loss: 1.3103154348604606, G Loss: 0.33094112258968933\nEpoch [477/1000], D Loss: 1.3167487733291856, G Loss: 0.3350917648185383\nEpoch [478/1000], D Loss: 1.2921791950861612, G Loss: 0.33515222758957836\nEpoch [479/1000], D Loss: 1.300034652334271, G Loss: 0.33910137848420574\nEpoch [480/1000], D Loss: 1.2942136912634878, G Loss: 0.33549688693248864\nEpoch [481/1000], D Loss: 1.2917524605086355, G Loss: 0.3365119654120821\nEpoch [482/1000], D Loss: 1.2990508921218642, G Loss: 0.33749121608156146\nEpoch [483/1000], D Loss: 1.3012763713345383, G Loss: 0.3324693887522726\nEpoch [484/1000], D Loss: 1.3117679270831022, G Loss: 0.33207029375162994\nEpoch [485/1000], D Loss: 1.302200116894462, G Loss: 0.3376270510933616\nEpoch [486/1000], D Loss: 1.2968289826855515, G Loss: 0.3369618740948764\nEpoch [487/1000], D Loss: 1.296254405108365, G Loss: 0.33580267718344026\nEpoch [488/1000], D Loss: 1.2979345014601043, G Loss: 0.33378091617064043\nEpoch [489/1000], D Loss: 1.3156541091023068, G Loss: 0.33185632626215616\nEpoch [490/1000], D Loss: 1.310712053197803, G Loss: 0.3326972849441297\nEpoch [491/1000], D Loss: 1.3044238412018978, G Loss: 0.3352224360812794\nEpoch [492/1000], D Loss: 1.3049996928735212, G Loss: 0.33331102439851473\nEpoch [493/1000], D Loss: 1.3115900140820127, G Loss: 0.33185681429776276\nEpoch [494/1000], D Loss: 1.2797661376721932, G Loss: 0.34778750827818206\nEpoch [495/1000], D Loss: 1.291903213298682, G Loss: 0.3377258528362621\nEpoch [496/1000], D Loss: 1.3016474778001959, G Loss: 0.3357190282055826\nEpoch [497/1000], D Loss: 1.299822491949255, G Loss: 0.3356759813698855\nEpoch [498/1000], D Loss: 1.2932867360837532, G Loss: 0.34000700585769883\nEpoch [499/1000], D Loss: 1.282620043104345, G Loss: 0.3356402529008461\nEpoch [500/1000], D Loss: 1.2834548818342613, G Loss: 0.3403019585392692\nEpoch [501/1000], D Loss: 1.2953037720738034, G Loss: 0.3322317620118459\nEpoch [502/1000], D Loss: 1.2923898642713374, G Loss: 0.3449562806071657\nEpoch [503/1000], D Loss: 1.2970918745705575, G Loss: 0.3377751391945463\nEpoch [504/1000], D Loss: 1.2928369854435777, G Loss: 0.3360236695318511\nEpoch [505/1000], D Loss: 1.3090697393272863, G Loss: 0.3328805520679011\nEpoch [506/1000], D Loss: 1.312195854476004, G Loss: 0.3349904972495455\nEpoch [507/1000], D Loss: 1.3156864108461321, G Loss: 0.3310596825498523\nEpoch [508/1000], D Loss: 1.2795286872170188, G Loss: 0.33923232645699475\nEpoch [509/1000], D Loss: 1.2829918160583034, G Loss: 0.3376642604668935\nEpoch [510/1000], D Loss: 1.312184719244639, G Loss: 0.3311905407544338\nEpoch [511/1000], D Loss: 1.3004323453614206, G Loss: 0.3343274582516063\nEpoch [512/1000], D Loss: 1.3146511276563009, G Loss: 0.3347019031192317\nEpoch [513/1000], D Loss: 1.3056979648994678, G Loss: 0.3354120166012735\nEpoch [514/1000], D Loss: 1.270731028282281, G Loss: 0.3453622545256759\nEpoch [515/1000], D Loss: 1.3043701563820695, G Loss: 0.33678643089352234\nEpoch [516/1000], D Loss: 1.2774874582435145, G Loss: 0.3412010079080408\nEpoch [517/1000], D Loss: 1.2994357986883684, G Loss: 0.33595405094551317\nEpoch [518/1000], D Loss: 1.3131737864378727, G Loss: 0.33264650413484287\nEpoch [519/1000], D Loss: 1.309456307960279, G Loss: 0.3327568079486038\nEpoch [520/1000], D Loss: 1.3057139028202405, G Loss: 0.3339679969079567\nEpoch [521/1000], D Loss: 1.3256301720937094, G Loss: 0.3317879606376995\nEpoch [522/1000], D Loss: 1.3127947251001995, G Loss: 0.3305423620975379\nEpoch [523/1000], D Loss: 1.3274320721626283, G Loss: 0.333888434641289\nEpoch [524/1000], D Loss: 1.2967467022664618, G Loss: 0.3330626796592366\nEpoch [525/1000], D Loss: 1.3070795778072242, G Loss: 0.3364338193878983\nEpoch [526/1000], D Loss: 1.2864946780782758, G Loss: 0.3563794150496974\nEpoch [527/1000], D Loss: 1.3078614393870036, G Loss: 0.3330533667044206\nEpoch [528/1000], D Loss: 1.2983433105728843, G Loss: 0.33396107175133444\nEpoch [529/1000], D Loss: 1.3095345110604257, G Loss: 0.33240584044745475\nEpoch [530/1000], D Loss: 1.3207214377143166, G Loss: 0.32984206712607184\nEpoch [531/1000], D Loss: 1.322843237356706, G Loss: 0.32938484278592195\nEpoch [532/1000], D Loss: 1.2889557842052344, G Loss: 0.34063836534818015\nEpoch [533/1000], D Loss: 1.3092642115824151, G Loss: 0.33237540396777066\nEpoch [534/1000], D Loss: 1.3008628810897018, G Loss: 0.3347797706271663\nEpoch [535/1000], D Loss: 1.3162178487488718, G Loss: 0.33070917563004926\nEpoch [536/1000], D Loss: 1.313524733167706, G Loss: 0.3307123238390142\nEpoch [537/1000], D Loss: 1.3143164497433286, G Loss: 0.33103306257363524\nEpoch [538/1000], D Loss: 1.3111479159557458, G Loss: 0.33327139616012574\nEpoch [539/1000], D Loss: 1.2991412177230373, G Loss: 0.3375282468217792\nEpoch [540/1000], D Loss: 1.3000553965568542, G Loss: 0.33714384924281726\nEpoch [541/1000], D Loss: 1.3148776885234947, G Loss: 0.33498356739679974\nEpoch [542/1000], D Loss: 1.2778330860715923, G Loss: 0.3446931698105552\nEpoch [543/1000], D Loss: 1.2818674246470134, G Loss: 0.33862238706964437\nEpoch [544/1000], D Loss: 1.272668972159877, G Loss: 0.3473978033571532\nEpoch [545/1000], D Loss: 1.3013204404802032, G Loss: 0.33565137440508064\nEpoch [546/1000], D Loss: 1.2969767899224252, G Loss: 0.33787711930997444\nEpoch [547/1000], D Loss: 1.297736889665777, G Loss: 0.3373753957676165\nEpoch [548/1000], D Loss: 1.3017600651943322, G Loss: 0.336090652328549\nEpoch [549/1000], D Loss: 1.2933765537811048, G Loss: 0.33765437639120854\nEpoch [550/1000], D Loss: 1.2977859818574153, G Loss: 0.33274308876557784\nEpoch [551/1000], D Loss: 1.3168374220530192, G Loss: 0.33182197321544993\nEpoch [552/1000], D Loss: 1.2963378335490372, G Loss: 0.33774368835218027\nEpoch [553/1000], D Loss: 1.3191789504253504, G Loss: 0.32910110011245264\nEpoch [554/1000], D Loss: 1.312946439150608, G Loss: 0.33404400131919165\nEpoch [555/1000], D Loss: 1.3137667070735584, G Loss: 0.3335072873216687\nEpoch [556/1000], D Loss: 1.3146031827637643, G Loss: 0.3316753062334928\nEpoch [557/1000], D Loss: 1.2840769296342676, G Loss: 0.3395325407837376\nEpoch [558/1000], D Loss: 1.308944938399575, G Loss: 0.3313202587040988\nEpoch [559/1000], D Loss: 1.3102325562274817, G Loss: 0.3346881367943504\nEpoch [560/1000], D Loss: 1.301697392535932, G Loss: 0.33653669303113765\nEpoch [561/1000], D Loss: 1.2898100126873364, G Loss: 0.3432602927540288\nEpoch [562/1000], D Loss: 1.3114639437559878, G Loss: 0.33272098302841185\nEpoch [563/1000], D Loss: 1.2761327190832659, G Loss: 0.33794999501921913\nEpoch [570/1000], D Loss: 1.3228092327262415, G Loss: 0.3313663531433452\nEpoch [571/1000], D Loss: 1.3102303822835286, G Loss: 0.33220578450145144\nEpoch [572/1000], D Loss: 1.3161485307144396, G Loss: 0.3320900346293594\nEpoch [573/1000], D Loss: 1.3073999852845164, G Loss: 0.33474745443373016\nEpoch [574/1000], D Loss: 1.2986135305780353, G Loss: 0.33832349704973624\nEpoch [575/1000], D Loss: 1.2768409797639557, G Loss: 0.3416145440303918\nEpoch [576/1000], D Loss: 1.3013722622033321, G Loss: 0.3349857160539338\nEpoch [577/1000], D Loss: 1.289587558038307, G Loss: 0.33511687751972313\nEpoch [578/1000], D Loss: 1.2877452156760476, G Loss: 0.33827383626591073\nEpoch [579/1000], D Loss: 1.310562092968912, G Loss: 0.33113395228530423\nEpoch [580/1000], D Loss: 1.2964702436418245, G Loss: 0.33143083290620284\nEpoch [581/1000], D Loss: 1.3229416131973266, G Loss: 0.3299900472164154\nEpoch [582/1000], D Loss: 1.3232364278851134, G Loss: 0.32846770268498043\nEpoch [583/1000], D Loss: 1.3138465805487198, G Loss: 0.3342377864953243\nEpoch [584/1000], D Loss: 1.3032704573689085, G Loss: 0.33158875956679834\nEpoch [585/1000], D Loss: 1.312842321395874, G Loss: 0.33287023883877376\nEpoch [586/1000], D Loss: 1.294534486351591, G Loss: 0.3383039922425241\nEpoch [587/1000], D Loss: 1.3236683827458007, G Loss: 0.33188577619465914\nEpoch [588/1000], D Loss: 1.2972543922337618, G Loss: 0.33999596509066493\nEpoch [589/1000], D Loss: 1.299109355247382, G Loss: 0.33638031988432915\nEpoch [590/1000], D Loss: 1.2991509343638565, G Loss: 0.33594405831712665\nEpoch [591/1000], D Loss: 1.2884789018919973, G Loss: 0.3381644996729764\nEpoch [592/1000], D Loss: 1.2979194471330353, G Loss: 0.33511100862965437\nEpoch [593/1000], D Loss: 1.2938338261662108, G Loss: 0.3362027361537471\nEpoch [594/1000], D Loss: 1.284861143430074, G Loss: 0.33779506304047324\nEpoch [595/1000], D Loss: 1.3038383913762641, G Loss: 0.33317532485181633\nEpoch [596/1000], D Loss: 1.2838877992196516, G Loss: 0.34455441055875835\nEpoch [597/1000], D Loss: 1.2987178679668543, G Loss: 0.33595578146703314\nEpoch [598/1000], D Loss: 1.2948572870456811, G Loss: 0.3348313340635011\nEpoch [599/1000], D Loss: 1.3107413183559071, G Loss: 0.33181624087420375\nEpoch [600/1000], D Loss: 1.3226140401580118, G Loss: 0.3319634703072635\nEpoch [601/1000], D Loss: 1.3224100582527392, G Loss: 0.33119945923487343\nEpoch [602/1000], D Loss: 1.3170494787620775, G Loss: 0.33240262215787714\nEpoch [603/1000], D Loss: 1.3005651799115268, G Loss: 0.33219412818099514\nEpoch [604/1000], D Loss: 1.3079722263596274, G Loss: 0.33317231409477466\nEpoch [605/1000], D Loss: 1.2824867916829659, G Loss: 0.3400704790245403\nEpoch [606/1000], D Loss: 1.300181099501523, G Loss: 0.33355379790970774\nEpoch [607/1000], D Loss: 1.2954343593481816, G Loss: 0.33513579802079635\nEpoch [608/1000], D Loss: 1.2962638959740147, G Loss: 0.33585118597204033\nEpoch [609/1000], D Loss: 1.3052104328617906, G Loss: 0.3345099909739061\nEpoch [610/1000], D Loss: 1.3119312755989305, G Loss: 0.3309678720705437\nEpoch [611/1000], D Loss: 1.3000734105254665, G Loss: 0.33423044392556855\nEpoch [612/1000], D Loss: 1.30945156993288, G Loss: 0.3300746354189786\nEpoch [613/1000], D Loss: 1.310475761240179, G Loss: 0.3328028274304939\nEpoch [614/1000], D Loss: 1.3039245851112136, G Loss: 0.3322832055164106\nEpoch [615/1000], D Loss: 1.3165396043748567, G Loss: 0.3316771463914351\nEpoch [616/1000], D Loss: 1.3238957397865527, G Loss: 0.33167659044265746\nEpoch [617/1000], D Loss: 1.3048374024304477, G Loss: 0.33244074330185397\nEpoch [618/1000], D Loss: 1.308086079900915, G Loss: 0.33264881824002124\nEpoch [619/1000], D Loss: 1.2892853422598405, G Loss: 0.3367047824642875\nEpoch [620/1000], D Loss: 1.3053286801684987, G Loss: 0.3321956943381916\nEpoch [621/1000], D Loss: 1.300926444385991, G Loss: 0.3325806200504303\nEpoch [622/1000], D Loss: 1.295033578077952, G Loss: 0.33973282846537506\nEpoch [623/1000], D Loss: 1.3104856910127582, G Loss: 0.3298791381445798\nEpoch [624/1000], D Loss: 1.3138889767906883, G Loss: 0.33193669427524913\nEpoch [625/1000], D Loss: 1.2862003593733817, G Loss: 0.3398369478456902\nEpoch [626/1000], D Loss: 1.3081010139349736, G Loss: 0.33214763529372937\nEpoch [627/1000], D Loss: 1.3124690966172652, G Loss: 0.3285220545349699\nEpoch [628/1000], D Loss: 1.29076577352755, G Loss: 0.33648047880692916\nEpoch [629/1000], D Loss: 1.2555926920789662, G Loss: 0.34674032709815283\nEpoch [630/1000], D Loss: 1.2922993154236764, G Loss: 0.3341696247910008\nEpoch [631/1000], D Loss: 1.318079124436234, G Loss: 0.3336904489632809\nEpoch [632/1000], D Loss: 1.3149272514112067, G Loss: 0.3310843496611624\nEpoch [633/1000], D Loss: 1.3262457320184418, G Loss: 0.32825553164337623\nEpoch [634/1000], D Loss: 1.3130864710518808, G Loss: 0.33047224933450875\nEpoch [635/1000], D Loss: 1.2896887255437446, G Loss: 0.3351777692635854\nEpoch [636/1000], D Loss: 1.2828141011975027, G Loss: 0.3376680207974983\nEpoch [637/1000], D Loss: 1.3043592442165721, G Loss: 0.3342633758530472\nEpoch [638/1000], D Loss: 1.2750668713540743, G Loss: 0.3366069638367855\nEpoch [639/1000], D Loss: 1.3037021593614058, G Loss: 0.3313001117923043\nEpoch [640/1000], D Loss: 1.2930583202477657, G Loss: 0.336194803498008\nEpoch [641/1000], D Loss: 1.2939781102267178, G Loss: 0.3358769172971899\nEpoch [642/1000], D Loss: 1.3084327791676378, G Loss: 0.3320338646570841\nEpoch [643/1000], D Loss: 1.296708468957381, G Loss: 0.33237648100563977\nEpoch [644/1000], D Loss: 1.2906597722660411, G Loss: 0.3394661180900805\nEpoch [645/1000], D Loss: 1.299890313726483, G Loss: 0.3352396609205188\nEpoch [646/1000], D Loss: 1.3068633874257405, G Loss: 0.3331238309542338\nEpoch [647/1000], D Loss: 1.3082366383436954, G Loss: 0.33275743795163704\nEpoch [648/1000], D Loss: 1.3193783131512729, G Loss: 0.3299416144688924\nEpoch [649/1000], D Loss: 1.3161124738779935, G Loss: 0.3334436604470918\nEpoch [650/1000], D Loss: 1.2964963844328217, G Loss: 0.33513291442033016\nEpoch [651/1000], D Loss: 1.292708604624777, G Loss: 0.33359710827018274\nEpoch [652/1000], D Loss: 1.3072840690612793, G Loss: 0.3309671683744951\nEpoch [653/1000], D Loss: 1.3011427799860635, G Loss: 0.3315915781440157\nEpoch [654/1000], D Loss: 1.3140539985714537, G Loss: 0.330989745349595\nEpoch [655/1000], D Loss: 1.2810515602429708, G Loss: 0.336704809737928\nEpoch [656/1000], D Loss: 1.3049649386694937, G Loss: 0.32991600903597745\nEpoch [657/1000], D Loss: 1.3080718419768593, G Loss: 0.336919802607912\nEpoch [658/1000], D Loss: 1.2987703442573548, G Loss: 0.3340163306756453\nEpoch [659/1000], D Loss: 1.3007237322402723, G Loss: 0.331053629246625\nEpoch [660/1000], D Loss: 1.305482606454329, G Loss: 0.33401475205565945\nEpoch [661/1000], D Loss: 1.281394236015551, G Loss: 0.33591834993073433\nEpoch [662/1000], D Loss: 1.2801678469686797, G Loss: 0.3428031045379061\nEpoch [663/1000], D Loss: 1.3060081698677757, G Loss: 0.33387318994059706\nEpoch [664/1000], D Loss: 1.3190390464031334, G Loss: 0.33164527199485083\nEpoch [665/1000], D Loss: 1.3007973053238608, G Loss: 0.3298374004436262\nEpoch [666/1000], D Loss: 1.3045738910183762, G Loss: 0.3339293158415592\nEpoch [667/1000], D Loss: 1.3026978261543043, G Loss: 0.33804907943263196\nEpoch [668/1000], D Loss: 1.2988580570076451, G Loss: 0.3341119327328422\nEpoch [669/1000], D Loss: 1.2850549340248107, G Loss: 0.34337491356965266\nEpoch [670/1000], D Loss: 1.2909691658886997, G Loss: 0.34307643391869286\nEpoch [671/1000], D Loss: 1.3140787124633788, G Loss: 0.32923819729776094\nEpoch [672/1000], D Loss: 1.3070657874598648, G Loss: 0.33769834005471433\nEpoch [673/1000], D Loss: 1.3104916579795607, G Loss: 0.3315709023764639\nEpoch [674/1000], D Loss: 1.303664336421273, G Loss: 0.3322435444051569\nEpoch [675/1000], D Loss: 1.324791350509181, G Loss: 0.3301330114855911\nEpoch [676/1000], D Loss: 1.316939914226532, G Loss: 0.3291101459300879\nEpoch [677/1000], D Loss: 1.3098123344508084, G Loss: 0.33200862299312245\nEpoch [678/1000], D Loss: 1.3054135557376978, G Loss: 0.33224787892717306\nEpoch [679/1000], D Loss: 1.302633848696044, G Loss: 0.33380804856618246\nEpoch [680/1000], D Loss: 1.3127140005429585, G Loss: 0.33205972476439044\nEpoch [681/1000], D Loss: 1.314910022056464, G Loss: 0.33087793841506496\nEpoch [682/1000], D Loss: 1.3186316287878788, G Loss: 0.32958680897048026\nEpoch [683/1000], D Loss: 1.290445332816153, G Loss: 0.3386895277283408\nEpoch [684/1000], D Loss: 1.301904356840885, G Loss: 0.33169465841669027\nEpoch [685/1000], D Loss: 1.3088430393825878, G Loss: 0.33418247266249224\nEpoch [686/1000], D Loss: 1.3148005272402907, G Loss: 0.3321107215953596\nEpoch [687/1000], D Loss: 1.2971074729254752, G Loss: 0.3376454425580574\nEpoch [688/1000], D Loss: 1.3206397081866408, G Loss: 0.329558483578942\nEpoch [689/1000], D Loss: 1.3197990944891265, G Loss: 0.3313303326115464\nEpoch [690/1000], D Loss: 1.2780178340998563, G Loss: 0.34485160076256954\nEpoch [691/1000], D Loss: 1.2999827059832487, G Loss: 0.3329651576099974\nEpoch [692/1000], D Loss: 1.3065264737967288, G Loss: 0.3311828046134024\nEpoch [693/1000], D Loss: 1.305209841150226, G Loss: 0.3304506222407023\nEpoch [694/1000], D Loss: 1.2871007287141047, G Loss: 0.3326828822945104\nEpoch [695/1000], D Loss: 1.281181860150713, G Loss: 0.34192601948073414\nEpoch [696/1000], D Loss: 1.3114495114846663, G Loss: 0.33109976396416174\nEpoch [697/1000], D Loss: 1.3054510140057767, G Loss: 0.33323746811259874\nEpoch [698/1000], D Loss: 1.3132541266354647, G Loss: 0.32951647151600233\nEpoch [699/1000], D Loss: 1.2954292965657783, G Loss: 0.33275237968473725\nEpoch [700/1000], D Loss: 1.3093815048535664, G Loss: 0.32926562934210807\nEpoch [701/1000], D Loss: 1.3105019901738022, G Loss: 0.3318424884117011\nEpoch [702/1000], D Loss: 1.2945062590367866, G Loss: 0.33491634383346097\nEpoch [703/1000], D Loss: 1.2861652742732654, G Loss: 0.33401201638308436\nEpoch [704/1000], D Loss: 1.2857345003070253, G Loss: 0.33798573793786946\nEpoch [705/1000], D Loss: 1.3148166728742194, G Loss: 0.33282450040181477\nEpoch [706/1000], D Loss: 1.28878264354937, G Loss: 0.3371354518514691\nEpoch [707/1000], D Loss: 1.2860889366178803, G Loss: 0.3379080273888328\nEpoch [708/1000], D Loss: 1.30323747251973, G Loss: 0.33204446453036685\nEpoch [709/1000], D Loss: 1.2946045658805154, G Loss: 0.33311817609902583\nEpoch [710/1000], D Loss: 1.297921813618053, G Loss: 0.33173200939640857\nEpoch [711/1000], D Loss: 1.3083345767223473, G Loss: 0.3301188096855626\nEpoch [712/1000], D Loss: 1.3005109628041585, G Loss: 0.33161843599695146\nEpoch [713/1000], D Loss: 1.305143479867415, G Loss: 0.3315329580595999\nEpoch [714/1000], D Loss: 1.2981035810528379, G Loss: 0.3322599698196758\nEpoch [715/1000], D Loss: 1.3039888920206013, G Loss: 0.3331234959038821\nEpoch [716/1000], D Loss: 1.2756462490919864, G Loss: 0.33835507161689526\nEpoch [717/1000], D Loss: 1.2967374975031072, G Loss: 0.33331086924581815\nEpoch [718/1000], D Loss: 1.3103385253386064, G Loss: 0.33048930782260316\nEpoch [719/1000], D Loss: 1.317792348428206, G Loss: 0.33246206243832904\nEpoch [720/1000], D Loss: 1.314847840684833, G Loss: 0.3296127601103349\nEpoch [721/1000], D Loss: 1.314544510118889, G Loss: 0.3294766935435208\nEpoch [722/1000], D Loss: 1.303308910673315, G Loss: 0.33233015501137936\nEpoch [723/1000], D Loss: 1.3025915149486427, G Loss: 0.33270180514364533\nEpoch [724/1000], D Loss: 1.29033281658635, G Loss: 0.334796289061055\nEpoch [725/1000], D Loss: 1.308800180753072, G Loss: 0.3313046872615814\nEpoch [726/1000], D Loss: 1.3027692025358026, G Loss: 0.3332813754226222\nEpoch [727/1000], D Loss: 1.2912695906379006, G Loss: 0.3377195782733686\nEpoch [728/1000], D Loss: 1.304636171731082, G Loss: 0.3317261296691317\nEpoch [729/1000], D Loss: 1.3095957308104544, G Loss: 0.3324639403458798\nEpoch [730/1000], D Loss: 1.304669935053045, G Loss: 0.3298579651297945\nEpoch [731/1000], D Loss: 1.301912905591907, G Loss: 0.33573845209497394\nEpoch [732/1000], D Loss: 1.3026943004492557, G Loss: 0.33366139007337164\nEpoch [733/1000], D Loss: 1.312033932136767, G Loss: 0.32995127258878765\nEpoch [734/1000], D Loss: 1.2943863532759927, G Loss: 0.3349247179248116\nEpoch [735/1000], D Loss: 1.2830148422356809, G Loss: 0.33955334370786494\nEpoch [736/1000], D Loss: 1.3066997484727338, G Loss: 0.3312451942400499\nEpoch [737/1000], D Loss: 1.271654292308923, G Loss: 0.34287594159444174\nEpoch [738/1000], D Loss: 1.3086813150030194, G Loss: 0.33046605135455276\nEpoch [739/1000], D Loss: 1.302109052918174, G Loss: 0.3328103562196096\nEpoch [740/1000], D Loss: 1.2936149134780421, G Loss: 0.33700762752330665\nEpoch [741/1000], D Loss: 1.2956035325021455, G Loss: 0.33370378974712256\nEpoch [742/1000], D Loss: 1.2813058125250267, G Loss: 0.3413713325153698\nEpoch [743/1000], D Loss: 1.297946372537902, G Loss: 0.3363081068703623\nEpoch [744/1000], D Loss: 1.3021818070700675, G Loss: 0.335835275144288\nEpoch [745/1000], D Loss: 1.3079286961844474, G Loss: 0.33055467135978467\nEpoch [746/1000], D Loss: 1.304808095368472, G Loss: 0.3313392868547729\nEpoch [747/1000], D Loss: 1.3214316548723164, G Loss: 0.3306817676081802\nEpoch [748/1000], D Loss: 1.3056645866596337, G Loss: 0.33239510438658976\nEpoch [749/1000], D Loss: 1.302031069090872, G Loss: 0.33405257138338956\nEpoch [750/1000], D Loss: 1.2868265213388386, G Loss: 0.3376932783560319\nEpoch [751/1000], D Loss: 1.3172605659022476, G Loss: 0.33124267603411817\nEpoch [752/1000], D Loss: 1.3141806002819176, G Loss: 0.32930809476158834\nEpoch [753/1000], D Loss: 1.3248870473919492, G Loss: 0.3299782342983015\nEpoch [754/1000], D Loss: 1.2991876558824018, G Loss: 0.33733427849682895\nEpoch [755/1000], D Loss: 1.2854357040289677, G Loss: 0.34007350314747203\nEpoch [756/1000], D Loss: 1.3037594184730992, G Loss: 0.3313452314246785\nEpoch [757/1000], D Loss: 1.296599117192355, G Loss: 0.33681819529244394\nEpoch [758/1000], D Loss: 1.3019733721559699, G Loss: 0.33254876100655756\nEpoch [759/1000], D Loss: 1.3084240407654733, G Loss: 0.33069745898246766\nEpoch [760/1000], D Loss: 1.3100637927199854, G Loss: 0.332526537324443\nEpoch [761/1000], D Loss: 1.299568857568683, G Loss: 0.3325188586206147\nEpoch [762/1000], D Loss: 1.3100274338866724, G Loss: 0.32936907374497615\nEpoch [763/1000], D Loss: 1.2990209019545353, G Loss: 0.3299736868251454\nEpoch [764/1000], D Loss: 1.3056408643722535, G Loss: 0.3317808064547452\nEpoch [765/1000], D Loss: 1.2869872880704476, G Loss: 0.3357710639635722\nEpoch [766/1000], D Loss: 1.274815088149273, G Loss: 0.3499848447062752\nEpoch [767/1000], D Loss: 1.2390829035730073, G Loss: 0.35253561410036954\nEpoch [768/1000], D Loss: 1.2948873429587393, G Loss: 0.3349596010916161\nEpoch [769/1000], D Loss: 1.282656062371803, G Loss: 0.34042566924384143\nEpoch [770/1000], D Loss: 1.3080890467672637, G Loss: 0.3301753703391913\nEpoch [771/1000], D Loss: 1.2913752010374357, G Loss: 0.3340215345223745\nEpoch [772/1000], D Loss: 1.3045374209230596, G Loss: 0.3307709408528877\nEpoch [773/1000], D Loss: 1.313796420169599, G Loss: 0.3309134300911065\nEpoch [774/1000], D Loss: 1.3111574736508456, G Loss: 0.3284679960120808\nEpoch [775/1000], D Loss: 1.3069779764522205, G Loss: 0.33263717673041604\nEpoch [776/1000], D Loss: 1.3048791166507836, G Loss: 0.3367831015225613\nEpoch [777/1000], D Loss: 1.2937611587119826, G Loss: 0.3350383429816275\nEpoch [778/1000], D Loss: 1.3107996846690322, G Loss: 0.33078857895099756\nEpoch [779/1000], D Loss: 1.3162065679376775, G Loss: 0.33063697905251477\nEpoch [780/1000], D Loss: 1.290453638452472, G Loss: 0.3365910425330653\nEpoch [781/1000], D Loss: 1.2906703349315758, G Loss: 0.33094496203191354\nEpoch [782/1000], D Loss: 1.3000351707140605, G Loss: 0.3348849972089132\nEpoch [783/1000], D Loss: 1.3094690395124031, G Loss: 0.33056620449730845\nEpoch [784/1000], D Loss: 1.300637089844906, G Loss: 0.3336098616773432\nEpoch [785/1000], D Loss: 1.2975511081290967, G Loss: 0.3348214469172738\nEpoch [786/1000], D Loss: 1.313487301089547, G Loss: 0.33103053443359604\nEpoch [787/1000], D Loss: 1.3205738118200592, G Loss: 0.32944267536654614\nEpoch [788/1000], D Loss: 1.3035572095350785, G Loss: 0.33671560323599614\nEpoch [789/1000], D Loss: 1.284773752183625, G Loss: 0.335672052340074\nEpoch [790/1000], D Loss: 1.3109844576228749, G Loss: 0.3315153419971466\nEpoch [791/1000], D Loss: 1.2846377535299822, G Loss: 0.3484520218589089\nEpoch [792/1000], D Loss: 1.3153012311819827, G Loss: 0.3394386410713196\nEpoch [793/1000], D Loss: 1.3115740133054328, G Loss: 0.33876381209402373\nEpoch [794/1000], D Loss: 1.3136575001658815, G Loss: 0.3332328583254959\nEpoch [795/1000], D Loss: 1.325987161289562, G Loss: 0.32975313537048573\nEpoch [796/1000], D Loss: 1.3217919255747939, G Loss: 0.329812224706014\nEpoch [797/1000], D Loss: 1.2827306256149755, G Loss: 0.3419475967233831\nEpoch [798/1000], D Loss: 1.3000667821277272, G Loss: 0.3388128051252076\nEpoch [799/1000], D Loss: 1.2811167991522587, G Loss: 0.34246114546602424\nEpoch [800/1000], D Loss: 1.286423579490546, G Loss: 0.34293038230953793\nEpoch [801/1000], D Loss: 1.295846789894682, G Loss: 0.33444664695046167\nEpoch [802/1000], D Loss: 1.322995751915556, G Loss: 0.33069225892876136\nEpoch [803/1000], D Loss: 1.3171163847952179, G Loss: 0.3312148822076393\nEpoch [804/1000], D Loss: 1.3086314663742529, G Loss: 0.3317505831068212\nEpoch [805/1000], D Loss: 1.3149608760169058, G Loss: 0.33361147913065825\nEpoch [806/1000], D Loss: 1.3029113794818068, G Loss: 0.33431254842064595\nEpoch [807/1000], D Loss: 1.307467076995156, G Loss: 0.33243770870295436\nEpoch [808/1000], D Loss: 1.3067587693532308, G Loss: 0.33391475840048357\nEpoch [809/1000], D Loss: 1.3007636460390959, G Loss: 0.33192663138562983\nEpoch [810/1000], D Loss: 1.3071911392789899, G Loss: 0.330023869601163\nEpoch [811/1000], D Loss: 1.3110417517748747, G Loss: 0.3308490670088566\nEpoch [812/1000], D Loss: 1.3091379526889686, G Loss: 0.3319039967927066\nEpoch [813/1000], D Loss: 1.2857120481404392, G Loss: 0.3376635643568906\nEpoch [814/1000], D Loss: 1.3028478477940415, G Loss: 0.33393177046920314\nEpoch [815/1000], D Loss: 1.2945984229897007, G Loss: 0.33644392490386965\nEpoch [816/1000], D Loss: 1.2973004138830937, G Loss: 0.33593341906865437\nEpoch [817/1000], D Loss: 1.2888856916716604, G Loss: 0.3351773359558799\nEpoch [818/1000], D Loss: 1.3080436374201918, G Loss: 0.3343295202110753\nEpoch [819/1000], D Loss: 1.3081525795387499, G Loss: 0.33398113521662626\nEpoch [820/1000], D Loss: 1.3190255916479863, G Loss: 0.33317167939561787\nEpoch [821/1000], D Loss: 1.3051315788066749, G Loss: 0.3331408162911733\nEpoch [822/1000], D Loss: 1.3192667079694342, G Loss: 0.3296652530178879\nEpoch [823/1000], D Loss: 1.3244413007389415, G Loss: 0.3285333264957775\nEpoch [824/1000], D Loss: 1.2939191005446695, G Loss: 0.3448661374323296\nEpoch [825/1000], D Loss: 1.2946549722642609, G Loss: 0.3350818336009979\nEpoch [826/1000], D Loss: 1.3070305824279784, G Loss: 0.33202670848730836\nEpoch [827/1000], D Loss: 1.28281654697476, G Loss: 0.3363910015785333\nEpoch [828/1000], D Loss: 1.304156945329724, G Loss: 0.3298250734806061\nEpoch [829/1000], D Loss: 1.3144215056390474, G Loss: 0.3327402089581345\nEpoch [830/1000], D Loss: 1.286980841737805, G Loss: 0.33633602658907574\nEpoch [831/1000], D Loss: 1.3022024967453696, G Loss: 0.3332082732157274\nEpoch [832/1000], D Loss: 1.298839432181734, G Loss: 0.33065819613861314\nEpoch [833/1000], D Loss: 1.3123249754761204, G Loss: 0.33205312075036947\nEpoch [834/1000], D Loss: 1.311721167419896, G Loss: 0.3318751116593679\nEpoch [835/1000], D Loss: 1.3079685854189325, G Loss: 0.3329371840664835\nEpoch [836/1000], D Loss: 1.3036144863475452, G Loss: 0.33329943508812876\nEpoch [837/1000], D Loss: 1.286173168818156, G Loss: 0.33262017929192744\nEpoch [838/1000], D Loss: 1.3017706340009516, G Loss: 0.33349058736454357\nEpoch [839/1000], D Loss: 1.2920686158266934, G Loss: 0.3333666888150302\nEpoch [840/1000], D Loss: 1.2962961641224948, G Loss: 0.3359722522172061\nEpoch [841/1000], D Loss: 1.2988681879910555, G Loss: 0.3352385683493181\nEpoch [842/1000], D Loss: 1.3072555523930174, G Loss: 0.32962427283778334\nEpoch [843/1000], D Loss: 1.2992074287298954, G Loss: 0.3328478733698527\nEpoch [844/1000], D Loss: 1.2891834009777416, G Loss: 0.33707785498012194\nEpoch [845/1000], D Loss: 1.3053968158635225, G Loss: 0.33088615103201435\nEpoch [846/1000], D Loss: 1.310996974959518, G Loss: 0.33264553691401627\nEpoch [847/1000], D Loss: 1.3146318385095308, G Loss: 0.32838067827802714\nEpoch [848/1000], D Loss: 1.3062394723747717, G Loss: 0.3314633163538846\nEpoch [849/1000], D Loss: 1.3166042703570742, G Loss: 0.32966149377100396\nEpoch [850/1000], D Loss: 1.3086288210117456, G Loss: 0.329845634915612\nEpoch [851/1000], D Loss: 1.2874891129407016, G Loss: 0.339007758732998\nEpoch [852/1000], D Loss: 1.3006208315040126, G Loss: 0.33360776178764573\nEpoch [853/1000], D Loss: 1.2943395585724802, G Loss: 0.3344074153538906\nEpoch [854/1000], D Loss: 1.3157868392539747, G Loss: 0.3293422829021107\nEpoch [855/1000], D Loss: 1.3079991141955059, G Loss: 0.33189033739494556\nEpoch [856/1000], D Loss: 1.3117706125432795, G Loss: 0.3335757427143328\nEpoch [857/1000], D Loss: 1.2989324147051031, G Loss: 0.33008312474597584\nEpoch [858/1000], D Loss: 1.3048358075546496, G Loss: 0.3322130445278052\nEpoch [859/1000], D Loss: 1.2894803975567672, G Loss: 0.34284759199980536\nEpoch [860/1000], D Loss: 1.3042634945927245, G Loss: 0.33733402797670076\nEpoch [861/1000], D Loss: 1.2941507653756574, G Loss: 0.33701073653770214\nEpoch [862/1000], D Loss: 1.31616803299297, G Loss: 0.33093218568599586\nEpoch [863/1000], D Loss: 1.3002935980305528, G Loss: 0.32917994206601925\nEpoch [864/1000], D Loss: 1.32221253712972, G Loss: 0.3346159844687491\nEpoch [865/1000], D Loss: 1.314115127288934, G Loss: 0.33228365562178874\nEpoch [866/1000], D Loss: 1.311496542078076, G Loss: 0.3307682228810859\nEpoch [867/1000], D Loss: 1.3019797646638118, G Loss: 0.33384773369991416\nEpoch [868/1000], D Loss: 1.3068337411591502, G Loss: 0.3291794677575429\nEpoch [869/1000], D Loss: 1.324448943860603, G Loss: 0.32879465547474945\nEpoch [870/1000], D Loss: 1.298341911128073, G Loss: 0.3357010817889011\nEpoch [871/1000], D Loss: 1.314651455301227, G Loss: 0.3318409013025688\nEpoch [872/1000], D Loss: 1.291907849817565, G Loss: 0.33665338826901986\nEpoch [873/1000], D Loss: 1.3176834185918171, G Loss: 0.33365022323348303\nEpoch [874/1000], D Loss: 1.2695855845104564, G Loss: 0.34987839225566747\nEpoch [875/1000], D Loss: 1.2969505082477222, G Loss: 0.3326244654077472\nEpoch [876/1000], D Loss: 1.30773445223317, G Loss: 0.32997689192945306\nEpoch [877/1000], D Loss: 1.3075504516110277, G Loss: 0.33164618394591594\nEpoch [878/1000], D Loss: 1.3039378881454469, G Loss: 0.33480287447120205\nEpoch [879/1000], D Loss: 1.309894211364515, G Loss: 0.3298754466302467\nEpoch [880/1000], D Loss: 1.3091205474102137, G Loss: 0.332968296065475\nEpoch [881/1000], D Loss: 1.3128697749340172, G Loss: 0.3301570648496801\nEpoch [882/1000], D Loss: 1.2905586668939302, G Loss: 0.3341399064569762\nEpoch [883/1000], D Loss: 1.2859459248456089, G Loss: 0.33478325695702527\nEpoch [884/1000], D Loss: 1.2726645802006578, G Loss: 0.345952322808179\nEpoch [885/1000], D Loss: 1.3061733755198393, G Loss: 0.3330400011756203\nEpoch [886/1000], D Loss: 1.2994375236106641, G Loss: 0.3357647438844045\nEpoch [887/1000], D Loss: 1.3000024943640738, G Loss: 0.3317908534497926\nEpoch [888/1000], D Loss: 1.265930678988948, G Loss: 0.3480469622395255\nEpoch [889/1000], D Loss: 1.3029392383315346, G Loss: 0.33492918195146504\nEpoch [890/1000], D Loss: 1.315917759953123, G Loss: 0.3282366552136161\nEpoch [891/1000], D Loss: 1.2989571697784192, G Loss: 0.33265873898159376\nEpoch [892/1000], D Loss: 1.3039211934263055, G Loss: 0.3339873537872777\nEpoch [893/1000], D Loss: 1.300196264368115, G Loss: 0.3330292015364676\nEpoch [894/1000], D Loss: 1.3119767167351462, G Loss: 0.330130118673498\nEpoch [895/1000], D Loss: 1.305994975566864, G Loss: 0.3299685077233748\nEpoch [896/1000], D Loss: 1.2944598541115269, G Loss: 0.33462334636485935\nEpoch [897/1000], D Loss: 1.3000170758276275, G Loss: 0.3353350294358803\nEpoch [898/1000], D Loss: 1.306102611801841, G Loss: 0.33077075029864456\nEpoch [899/1000], D Loss: 1.307257886727651, G Loss: 0.32911599693876326\nEpoch [900/1000], D Loss: 1.3086409099174268, G Loss: 0.3295790917945631\nEpoch [901/1000], D Loss: 1.3076502366499467, G Loss: 0.32896237048235805\nEpoch [902/1000], D Loss: 1.3114102359974023, G Loss: 0.33074691891670227\nEpoch [903/1000], D Loss: 1.3061823801560835, G Loss: 0.3312459102182677\nEpoch [904/1000], D Loss: 1.312073957197594, G Loss: 0.33147475322087605\nEpoch [905/1000], D Loss: 1.3004236766786286, G Loss: 0.3348177330060439\nEpoch [906/1000], D Loss: 1.3073156078656514, G Loss: 0.3324785281311382\nEpoch [907/1000], D Loss: 1.2888664996985233, G Loss: 0.3336285423148762\nEpoch [908/1000], D Loss: 1.2976417776310083, G Loss: 0.34524891701611604\nEpoch [909/1000], D Loss: 1.3249239675926439, G Loss: 0.330814680186185\nEpoch [910/1000], D Loss: 1.31987226153865, G Loss: 0.33074871500333153\nEpoch [911/1000], D Loss: 1.3263365550474686, G Loss: 0.33163944789857575\nEpoch [912/1000], D Loss: 1.3255035169196852, G Loss: 0.32823838699947705\nEpoch [913/1000], D Loss: 1.313157284259796, G Loss: 0.3319735740170334\nEpoch [914/1000], D Loss: 1.3114770264336557, G Loss: 0.33137737386154403\nEpoch [915/1000], D Loss: 1.3091451681021489, G Loss: 0.3327694255294222\nEpoch [916/1000], D Loss: 1.2931699124249545, G Loss: 0.3307277005730253\nEpoch [917/1000], D Loss: 1.3104389118425774, G Loss: 0.3296716404683662\nEpoch [918/1000], D Loss: 1.305279881665201, G Loss: 0.3329178866111871\nEpoch [919/1000], D Loss: 1.3190339666424375, G Loss: 0.33064963853720464\nEpoch [920/1000], D Loss: 1.2952927354610329, G Loss: 0.3364480253421899\nEpoch [921/1000], D Loss: 1.303495495969599, G Loss: 0.33209547021172264\nEpoch [922/1000], D Loss: 1.3054342865943909, G Loss: 0.330384135426897\nEpoch [923/1000], D Loss: 1.3047988801291495, G Loss: 0.33418715957439304\nEpoch [924/1000], D Loss: 1.3085602120919662, G Loss: 0.33054108691937995\nEpoch [925/1000], D Loss: 1.307886552810669, G Loss: 0.3284519789796887\nEpoch [926/1000], D Loss: 1.3112259741985437, G Loss: 0.32985785513213184\nEpoch [927/1000], D Loss: 1.3073052247365315, G Loss: 0.33095822153669413\nEpoch [928/1000], D Loss: 1.3289133881077622, G Loss: 0.3271606344165224\nEpoch [929/1000], D Loss: 1.2991567673105182, G Loss: 0.3357726891835531\nEpoch [930/1000], D Loss: 1.306456775376291, G Loss: 0.33157365972345526\nEpoch [931/1000], D Loss: 1.3128883621909402, G Loss: 0.3316969430807865\nEpoch [932/1000], D Loss: 1.3137455232215651, G Loss: 0.329793022256909\nEpoch [933/1000], D Loss: 1.316889399470705, G Loss: 0.32774262482469735\nEpoch [934/1000], D Loss: 1.287963292092988, G Loss: 0.3362254285451138\nEpoch [935/1000], D Loss: 1.2914997259775798, G Loss: 0.3363395566290075\nEpoch [936/1000], D Loss: 1.2934304443272677, G Loss: 0.3377416095950387\nEpoch [937/1000], D Loss: 1.3168720801671345, G Loss: 0.3298854255315029\nEpoch [938/1000], D Loss: 1.302548885345459, G Loss: 0.3290775960141962\nEpoch [939/1000], D Loss: 1.2823247479669975, G Loss: 0.3361095728296222\nEpoch [940/1000], D Loss: 1.3187952785780936, G Loss: 0.32770753809899994\nEpoch [941/1000], D Loss: 1.3148019190990563, G Loss: 0.32978166323719604\nEpoch [942/1000], D Loss: 1.306172860029972, G Loss: 0.3316311957258167\nEpoch [943/1000], D Loss: 1.31394036321929, G Loss: 0.3332819508783745\nEpoch [944/1000], D Loss: 1.2915283936442752, G Loss: 0.3378259546828992\nEpoch [945/1000], D Loss: 1.3178656765908905, G Loss: 0.33261737046819745\nEpoch [946/1000], D Loss: 1.3232806317733996, G Loss: 0.3289835790793101\nEpoch [947/1000], D Loss: 1.3204975446065268, G Loss: 0.3321756343046824\nEpoch [948/1000], D Loss: 1.315180126103488, G Loss: 0.3303251266479492\nEpoch [949/1000], D Loss: 1.3200186877539664, G Loss: 0.33038630792588897\nEpoch [950/1000], D Loss: 1.3114519690022324, G Loss: 0.32921838326887654\nEpoch [951/1000], D Loss: 1.2907299840089046, G Loss: 0.33619455297787987\nEpoch [952/1000], D Loss: 1.3110001997514205, G Loss: 0.33603013320402664\nEpoch [953/1000], D Loss: 1.3104658701203087, G Loss: 0.33028716390783136\nEpoch [954/1000], D Loss: 1.3080841823057694, G Loss: 0.3304877971157883\nEpoch [955/1000], D Loss: 1.3053930560747782, G Loss: 0.33434517311327383\nEpoch [956/1000], D Loss: 1.3086698001081294, G Loss: 0.3301993823412693\nEpoch [957/1000], D Loss: 1.3202795693368623, G Loss: 0.329572166818561\nEpoch [958/1000], D Loss: 1.3208311442172889, G Loss: 0.32933974320238285\nEpoch [959/1000], D Loss: 1.3027523669329557, G Loss: 0.33469040393829347\nEpoch [960/1000], D Loss: 1.3011370023091635, G Loss: 0.3358743293718858\nEpoch [961/1000], D Loss: 1.310097625761321, G Loss: 0.32868327245567786\nEpoch [962/1000], D Loss: 1.2955676591757572, G Loss: 0.3359445819349\nEpoch [963/1000], D Loss: 1.3043049172921615, G Loss: 0.3339728051965887\nEpoch [964/1000], D Loss: 1.3152190602186955, G Loss: 0.33127714612267234\nEpoch [965/1000], D Loss: 1.3131309964440085, G Loss: 0.32917657350048873\nEpoch [966/1000], D Loss: 1.3235947569211324, G Loss: 0.3331214901172754\nEpoch [967/1000], D Loss: 1.3052313183293198, G Loss: 0.3345554592031421\nEpoch [968/1000], D Loss: 1.311874063087232, G Loss: 0.3312781855915532\nEpoch [969/1000], D Loss: 1.3141104806553234, G Loss: 0.33024596698356395\nEpoch [970/1000], D Loss: 1.29950224070838, G Loss: 0.3372504745468949\nEpoch [971/1000], D Loss: 1.298127007484436, G Loss: 0.33544394500327834\nEpoch [972/1000], D Loss: 1.297186915440993, G Loss: 0.34190883076552187\nEpoch [973/1000], D Loss: 1.2870790980078957, G Loss: 0.3395451666730823\nEpoch [974/1000], D Loss: 1.2987654277772613, G Loss: 0.3327243503296014\nEpoch [975/1000], D Loss: 1.3220844052054666, G Loss: 0.3290666274952166\nEpoch [976/1000], D Loss: 1.300803956118497, G Loss: 0.33050013076175344\nEpoch [977/1000], D Loss: 1.2907263506542552, G Loss: 0.33535259655027677\nEpoch [978/1000], D Loss: 1.3119787523240753, G Loss: 0.33317060452519043\nEpoch [979/1000], D Loss: 1.305550134904457, G Loss: 0.33276768507379473\nEpoch [980/1000], D Loss: 1.313534891244137, G Loss: 0.3342156813000188\nEpoch [981/1000], D Loss: 1.311426413781715, G Loss: 0.3290106742671042\nEpoch [982/1000], D Loss: 1.2731253750396498, G Loss: 0.33990288521304274\nEpoch [983/1000], D Loss: 1.299131455565944, G Loss: 0.32973888426116016\nEpoch [984/1000], D Loss: 1.2917439224142018, G Loss: 0.33821474133115825\nEpoch [985/1000], D Loss: 1.2927933909676292, G Loss: 0.3339683512846629\nEpoch [986/1000], D Loss: 1.3119239023237517, G Loss: 0.32909397862174294\nEpoch [987/1000], D Loss: 1.3090274738542962, G Loss: 0.3299859440687931\nEpoch [988/1000], D Loss: 1.3137828216408238, G Loss: 0.3285240698944439\nEpoch [989/1000], D Loss: 1.3167427438678163, G Loss: 0.3287675147706812\nEpoch [990/1000], D Loss: 1.2973231413147666, G Loss: 0.334831906448711\nEpoch [991/1000], D Loss: 1.3172675884131229, G Loss: 0.3282899670528643\nEpoch [992/1000], D Loss: 1.3081791072180777, G Loss: 0.3315410998734561\nEpoch [993/1000], D Loss: 1.308053500363321, G Loss: 0.3305033427296263\nEpoch [994/1000], D Loss: 1.3047428033568642, G Loss: 0.32883779352361503\nEpoch [995/1000], D Loss: 1.3038384486328471, G Loss: 0.3365994971809965\nEpoch [996/1000], D Loss: 1.308583104610443, G Loss: 0.33222995562986896\nEpoch [997/1000], D Loss: 1.3089339039542458, G Loss: 0.32878125020951937\nEpoch [998/1000], D Loss: 1.3123441436073997, G Loss: 0.328856785550262\nEpoch [999/1000], D Loss: 1.3239448525688864, G Loss: 0.32788183038884944\nEpoch [1000/1000], D Loss: 1.3044988335985126, G Loss: 0.3318470588236144\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import torch\nimport collections\nimport matplotlib.pyplot as plt\n\n# Set device to CPU or GPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Hardcoded number of synthetic images needed for each class\nsynthetic_images_needed = {\n    0: 0,    # Class 0 is already balanced\n    1: 1054, # Class 1 needs 1054 synthetic images\n    2: 613,  # Class 2 needs 613 synthetic images\n    3: 1176, # Class 3 needs 1176 synthetic images\n    4: 1109  # Class 4 needs 1109 synthetic images\n}\n\n# Function to visualize images\ndef visualize_images(images, title=\"Synthetic Images\"):\n    images = images.detach().cpu()\n    grid_size = int(len(images) ** 0.5)\n    fig, axes = plt.subplots(grid_size, grid_size, figsize=(10, 10))\n    for i, ax in enumerate(axes.flat):\n        if i < len(images):\n            img = (images[i] - images[i].min()) / (images[i].max() - images[i].min())  # Normalize to [0, 1]\n            ax.imshow(img.permute(1, 2, 0))  # Change channel order for visualization\n            ax.axis('off')\n        else:\n            ax.axis('off')\n    plt.suptitle(title, fontsize=16)\n    plt.show()\n\n# Function to generate synthetic images for a specific class\ndef generate_synthetic_images_for_class(generator, class_label, num_images, batch_size=8, noise_dim=100, visualize=False):\n    synthetic_images = []\n\n    # Ensure the generator is on the correct device\n    generator.to(device)\n    generator.eval()\n\n    # Generate synthetic images\n    for batch_idx in range((num_images + batch_size - 1) // batch_size):\n        noise = torch.randn(batch_size, noise_dim, device=device)\n        with torch.no_grad():\n            fake_images = generator(noise)\n\n        if len(fake_images) > num_images:\n            fake_images = fake_images[:num_images]\n\n        synthetic_images.append(fake_images)\n\n        if visualize and batch_idx == 0:  # Visualize the first batch\n            print(f\"Visualizing synthetic images for class {class_label}...\")\n            visualize_images(fake_images, title=f\"Class {class_label} Synthetic Images\")\n\n    synthetic_images = torch.cat(synthetic_images, dim=0)\n    return synthetic_images\n\n# Function to combine real and synthetic images\ndef combine_real_and_synthetic(train_dataset, synthetic_images_by_class, synthetic_images_needed):\n    augmented_images = []\n    augmented_labels = []\n\n    # Add real images from the original dataset\n    for image, label in train_dataset:\n        augmented_images.append(image.to(device))\n        augmented_labels.append(label)\n\n    # Add synthetic images for each class\n    for class_label, images_needed in synthetic_images_needed.items():\n        if images_needed > 0:\n            synthetic_images = synthetic_images_by_class.get(class_label, torch.Tensor()).to(device)\n            augmented_images.extend(synthetic_images[:images_needed])\n            augmented_labels.extend([class_label] * images_needed)\n\n    # Check class distribution\n    class_counts = collections.Counter(augmented_labels)\n    print(f\"Class distribution after combining: {class_counts}\")\n\n    # Create a new dataset\n    augmented_dataset = torch.utils.data.TensorDataset(\n        torch.stack(augmented_images), torch.tensor(augmented_labels).to(device)\n    )\n    return augmented_dataset\n\n# Main Execution\nsynthetic_images_by_class = {}\n\n# Generate synthetic images for each class\nfor class_label, images_needed in synthetic_images_needed.items():\n    if images_needed > 0:\n        print(f\"Generating {images_needed} synthetic images for class {class_label}...\")\n        synthetic_images = generate_synthetic_images_for_class(\n            generator, class_label, images_needed, visualize=True\n        )\n        synthetic_images_by_class[class_label] = synthetic_images\n\n# Combine real and synthetic datasets\nprint(\"Combining real and synthetic datasets...\")\nbalanced_train_dataset = combine_real_and_synthetic(train_dataset, synthetic_images_by_class, synthetic_images_needed)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the final balanced dataset to .pth file\ntorch.save(balanced_train_dataset, \"Balanced_train_DataSet.pth\")\nprint(\"Balanced train dataset saved to balanced_train_DataSet.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T18:37:48.912801Z","iopub.execute_input":"2025-01-04T18:37:48.913188Z","iopub.status.idle":"2025-01-04T18:37:57.884482Z","shell.execute_reply.started":"2025-01-04T18:37:48.913153Z","shell.execute_reply":"2025-01-04T18:37:57.882931Z"}},"outputs":[{"name":"stdout","text":"Balanced train dataset saved to balanced_train_DataSet.pth\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Generate and display the link\ndisplay(FileLink(r'Balanced_train_DataSet.pth'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T18:38:54.270749Z","iopub.execute_input":"2025-01-04T18:38:54.271108Z","iopub.status.idle":"2025-01-04T18:38:54.277177Z","shell.execute_reply.started":"2025-01-04T18:38:54.271081Z","shell.execute_reply":"2025-01-04T18:38:54.276224Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"/kaggle/working/Balanced_train_DataSet.pth","text/html":"<a href='Balanced_train_DataSet.pth' target='_blank'>Balanced_train_DataSet.pth</a><br>"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom collections import Counter\nimport torch\n\n# Function to count the images per class in a given dataset\ndef count_images_per_class(dataset):\n    class_counts = Counter()\n    for _, label in dataset:\n        if isinstance(label, torch.Tensor):  # Convert tensor label to scalar\n            label = label.item()\n        class_counts[label] += 1\n    return class_counts\n\n# Function to plot a bar chart for class distribution\ndef plot_class_distribution(class_counts, title=\"Class Distribution\"):\n    sorted_classes = sorted(class_counts.keys())\n    sorted_counts = [class_counts[c] for c in sorted_classes]\n\n    plt.figure(figsize=(10, 6))\n    plt.bar(sorted_classes, sorted_counts, color='skyblue')\n    plt.xlabel('Class', fontsize=12)\n    plt.ylabel('Number of Images', fontsize=12)\n    plt.title(title, fontsize=14)\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\n# # Verify the distribution of the real dataset\n# real_class_counts = count_images_per_class(train_dataset)\n# print(\"Real dataset class distribution:\", real_class_counts)\n# plot_class_distribution(real_class_counts, title=\"Class Distribution of Real Dataset\")\n\n# # Verify the distribution of synthetic dataset (if you stored synthetic images separately)\n# synthetic_class_counts = count_images_per_class(torch.utils.data.TensorDataset(\n#     torch.cat([v for v in synthetic_images_by_class.values()], dim=0),\n#     torch.cat([torch.tensor([k] * len(v)) for k, v in synthetic_images_by_class.items()])\n# ))\n# print(\"Synthetic dataset class distribution:\", synthetic_class_counts)\n# plot_class_distribution(synthetic_class_counts, title=\"Class Distribution of Synthetic Dataset\")\n\n# Verify the distribution of the combined dataset\ncombined_class_counts = count_images_per_class(balanced_train_dataset)\nprint(\"Combined dataset class distribution:\", combined_class_counts)\nplot_class_distribution(combined_class_counts, title=\"Class Distribution After Combining Datasets\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n# Load the balanced dataset\nbalanced_train_dataset = torch.load(\"the_balanced_train_dataset.pth\")\nprint(\"Balanced dataset loaded successfully!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import Counter\n\n# Extract all class labels from the dataset\nclass_labels = [label for _, label in balanced_train_dataset]\n\n# Count the occurrences of each class\nclass_counts = Counter(class_labels)\nprint(\"Class Distribution:\", class_counts)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Prepare data for plotting\nclasses = list(class_counts.keys())\ncounts = list(class_counts.values())\n\n# Create the bar chart\nplt.figure(figsize=(10, 6))\nplt.bar(classes, counts, color='skyblue', edgecolor='black')\n\n# Add labels and title\nplt.xlabel(\"Classes\")\nplt.ylabel(\"Number of Samples\")\nplt.title(\"Class Distribution in Balanced Dataset\")\nplt.xticks(classes)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\n# Display the plot\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T02:22:39.503715Z","iopub.execute_input":"2025-01-04T02:22:39.504381Z","iopub.status.idle":"2025-01-04T02:23:10.886964Z","shell.execute_reply.started":"2025-01-04T02:22:39.504348Z","shell.execute_reply":"2025-01-04T02:23:10.886107Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"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"},"metadata":{}}],"execution_count":34}]}